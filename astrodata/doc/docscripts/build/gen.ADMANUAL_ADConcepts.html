

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>4. Concepts &mdash; astrodata documentation v1.00 documentation</title>
    <link rel="stylesheet" href="_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '',
        VERSION:     '1.00',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="astrodata documentation v1.00 documentation" href="index.html" />
    <link rel="prev" title="<no title>" href="creatingAPrimitive.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="creatingAPrimitive.html" title="<no title>"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">astrodata documentation v1.00 documentation</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">4. Concepts</a><ul>
<li><a class="reference internal" href="#background">4.1. Background</a><ul>
<li><a class="reference internal" href="#dataset-abstraction">4.1.1. Dataset Abstraction</a></li>
<li><a class="reference internal" href="#dataset-transformations">4.1.2. Dataset Transformations</a><ul>
<li><a class="reference internal" href="#zero-recipe-system-overhead-for-astrodata-only-users">4.1.2.1. Zero Recipe System Overhead for AstroData-only Users</a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-astrodata-lexicon-and-configurations">4.1.3. The Astrodata Lexicon and Configurations</a></li>
</ul>
</li>
<li><a class="reference internal" href="#astrodata-type">4.2. Astrodata Type</a></li>
<li><a class="reference internal" href="#astrodata-descriptors">4.3. Astrodata Descriptors</a></li>
<li><a class="reference internal" href="#recipe-system-primitives">4.4. Recipe System Primitives</a><ul>
<li><a class="reference internal" href="#some-benefits-of-the-primitive-concept">4.4.1. Some Benefits of the Primitive Concept</a><ul>
<li><a class="reference internal" href="#natural-emergence-of-reusable-primitives">4.4.1.1. Natural Emergence of Reusable Primitives</a></li>
</ul>
</li>
<li><a class="reference internal" href="#recipes-calling-recipes">4.4.2. Recipes calling Recipes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#astrodata-lexicon">4.5. AstroData Lexicon</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="creatingAPrimitive.html"
                        title="previous chapter">&lt;no title&gt;</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/gen.ADMANUAL_ADConcepts.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" size="18" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="concepts">
<h1>4. Concepts<a class="headerlink" href="#concepts" title="Permalink to this headline">¶</a></h1>
<div class="section" id="background">
<h2>4.1. Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<div class="section" id="dataset-abstraction">
<h3>4.1.1. Dataset Abstraction<a class="headerlink" href="#dataset-abstraction" title="Permalink to this headline">¶</a></h3>
<p>The AstroData class traces back to a request by Gemini Astronomers to
&#8220;handle MEFs better&#8221; in our reduction package. A &#8220;MEF&#8221; is of course a
&#8220;Multiple-Extension FITS File&#8221; and is also Gemini&#8217;s standard dataset
storage format. Investigation showed that the MEF libraries were
sufficient for handling &#8220;MEFs&#8221; as such and the real meaning of the
request was for a better dataset abstraction for Gemini&#8217;s datasets.
Gemini MEFs, and MEFs in general, are usually meant to be coherent
collections of data; the separate pixel arrays, or extensions, are
collocated in a common MEF for that reason. The MEF abstraction itself
does not recognize these connections, however, and views the MEF as a
list of separate header-data units, their only relation being
collocation in the list. Even the PHU, which has certain artifacts as
a special header, generally not having pixel data, and which is used
as a file-wide header, is merely presented as the header-data unit at
index 0. AstroData relies on one pair of relational meta-data
available in MEF which is indexing of the list of datasets with
(EXTNAME, EXTVER) tuple.</p>
<p>FITS libraries (e.g. pyfits) return opened MEFs as objects which act
as lists of Header-Data Units, aka &#8220;extensions&#8221;. AstroData on the
other hand is designed to be configured to recognize many internal
connections that MEF does not directly encode. AstroData detects the
type of the data, and then can make sound assumptions about what the
data is and how to handle it. Any particular (python-level) actions on
the data are then performed by implementations in the configuration
space.</p>
<p>An additional role of the AstroData abstraction is to standardize
access to metadata. FITS allows copious metadata in each extension and
in the shared &#8220;zero&#8217;th&#8221; extension (aka &#8220;the PHU&#8221;), but it standardizes
only a small subset of what sort of information is stored there. Many
properties which are for Gemini essentially universal properties for
all of our datasets, across instruments and modes, are not
standardized by FITS. For different instruments and modes these bits
of information are distributed across different header key-value pairs
and stored in different units. This leads to a situation where there
is information that is in principle available in all datasets, but
which requires instrument-mode-specific coding to be retrieved in a
particular unit and with a particular technical meaning. AstroData
hides the particulars by allowing functions that calculate the
metadata to be defined in the same configuration space in which the
dataset type itself is defined.</p>
<p>The AstroDataType system is able to look at any aspect of the dataset
to judge if it belongs in a given classification, but the intent is to
find characteristics in the MEF&#8217;s PHU. Using this knowledge, AstroData
loads and applies particular instrument-mode-specific methods to
obtain general behavior through a uniform interface, as desired for
the user. This uniform interface can be presented not only in the case
of meta-data but also in the case of transformations and any other
dataset-type-specific behavior.</p>
<p>To first order, Astrodata Types map to instrument-modes, and these
provide a good concrete image of what Astrodata Type are. However more
abstract types of dataset identification are also possible and make
themselves useful, such as generic types such as &#8220;IFU&#8221; vs &#8220;IMAGE&#8221;, or
processing status types such as &#8220;RAW&#8221; vs &#8220;PREPARED&#8221;.</p>
</div>
<div class="section" id="dataset-transformations">
<h3>4.1.2. Dataset Transformations<a class="headerlink" href="#dataset-transformations" title="Permalink to this headline">¶</a></h3>
<p>The Astrodata package&#8217;s &#8220;Recipe System&#8221; handles all abstractions
involved in transforming a dataset and is built on top of the
AstroData dataset abstraction. The system is called the &#8220;recipe
system&#8221; because the top level instructions for transforming data are
&#8220;recipes&#8221;, text files of sequential instructions to perform. For
example the recipe &#8220;overscanCorrect&#8221; contains the following (comments
removed):</p>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><pre>1
2
3
4
5
6</pre></td><td class="code"><div class="highlight"><pre><span class="n">prepare</span>
<span class="n">overscanCorrect</span>
<span class="n">addVARDQ</span>
<span class="n">setStackable</span>
<span class="n">averageCombine</span>
<span class="n">storeProcessedBias</span><span class="p">(</span><span class="n">clob</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>Each of these instructions is either a &#8220;primitive&#8221;, which is a python
function implemented in the configuration space for a dataset of the
given classification, or another recipe. Note that the
&#8220;storeProcessedBias&#8221; primitive above takes an argument in this
example, &#8220;clob(ber)&#8221; equals &#8220;True&#8221;, which tells the storage primitive
to overwrite (clobber) any pre-existing bias of the same name.</p>
<div class="section" id="zero-recipe-system-overhead-for-astrodata-only-users">
<h4>4.1.2.1. Zero Recipe System Overhead for AstroData-only Users<a class="headerlink" href="#zero-recipe-system-overhead-for-astrodata-only-users" title="Permalink to this headline">¶</a></h4>
<p>Use of AstroData does NOT lead to importing any part of the &#8220;Recipe
System&#8221;. Thus there there is no overhead borne by users of the
AstroData dataset abstract if they do not specifically invoke the
Recipe System. Neither the configuration package nor even the related
astrodata package modules are imported until the Recipe System is
explicitly invoked by the calling program.</p>
</div>
</div>
<div class="section" id="the-astrodata-lexicon-and-configurations">
<h3>4.1.3. The Astrodata Lexicon and Configurations<a class="headerlink" href="#the-astrodata-lexicon-and-configurations" title="Permalink to this headline">¶</a></h3>
<p>An Astrodata Configuration package, defining types, metadata, and
transformations, relies on a configuration which defines a lexicon of
elements which are implemented in the configuration package in a way
such that Astrodata can load and apply the functionality involved. Put
simply a combination of location and naming conventions allows the
configuration author to define elements in a way that astrodata will
disover. In the current system there are three types of elements to be
concerned with:</p>
<ul class="simple">
<li>dataset classification names, aka <strong>Astrodata Types</strong></li>
<li>high level metadata names, aka <strong>Astrodata Descriptors</strong></li>
<li>scientifically meaningful discrete dataset transformation names, aka
<strong>Primitives</strong></li>
</ul>
<p>Each of these have associated actions:</p>
<ul class="simple">
<li><strong>Astrodata Type</strong>: checks a dataset for adherence to an AstroData
type classification criteria, generally by checking key-value pairs in
the PHU.</li>
<li><strong>Astrodata Descriptors</strong>: calculate and return a named piece of
high-level metadata for a particular Astrodata Type in particular
units.</li>
<li><strong>Primitives</strong>: performs a named transformation on a dataset of a
particular Astrodata Type.</li>
</ul>
<p>The &#8220;astrodata_Gemini&#8221; package contains these definitions for Gemini
datasets separated into two parts, one for the basic AstroData related
configuration information, and another for Recipe System
configuration. The first section, in its own subdirectory in the
configuration package directory, in Gemini&#8217;s case is found in the
ADCONFIG_Gemini configuration subdirectory. Configurations in this
subdirectory define types, descriptor functions, and other AstroData-
related features. The second section, in a sibling subdirectory in the
configuration package, in Gemini&#8217;s case, &#8220;RECIPES_Gemini&#8221;, defines
configurations and implementations needed by the Recipe System, such
as recipes and primitives.</p>
</div>
</div>
<div class="section" id="astrodata-type">
<h2>4.2. Astrodata Type<a class="headerlink" href="#astrodata-type" title="Permalink to this headline">¶</a></h2>
<p>An Astrodata Type is a named set of dataset characteristics.</p>
<p>Lack of a central system for type detection in our legacy package
meant that scripts and tasks in that system make extended checks on
the header data in the datasets they manipulate. Often these checks
merely verify that the right type of data is being worked on, a very
common task, yet these checks can still be somewhat complex and
brittle, for example relying on specific headers which may change when
an instrument is upgraded.</p>
<p>Astrodata&#8217;s classification system on the other hand allows defining
dataset classifications via configuration packaging such that the type
definitions are shared throughout the system. The calling code can
refer to type information by a string name for the type, and any
subtleties in or changes to the means of detection are centralized,
providing some forward and backward compatibility. The system also
allows programmers to check dataset types with a single line of code:</p>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11</pre></td><td class="code"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">astrodata.AstroData</span> <span class="kn">import</span> <span class="n">AstroData</span>

<span class="n">ad</span> <span class="o">=</span> <span class="n">AstroData</span><span class="p">(</span><span class="s">&quot;N20091027S0134.fits&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">ad</span><span class="o">.</span><span class="n">isType</span><span class="p">(</span><span class="s">&quot;GMOS_IMAGE&quot;</span><span class="p">):</span>
   <span class="n">gmos_specific_function</span><span class="p">(</span><span class="n">ad</span><span class="p">)</span>

<span class="k">if</span> <span class="n">ad</span><span class="o">.</span><span class="n">isType</span><span class="p">(</span><span class="s">&quot;RAW&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="bp">False</span><span class="p">:</span>
   <span class="k">print</span> <span class="s">&quot;Dataset is not RAW data, already processed.&quot;</span>
<span class="k">else</span><span class="p">:</span>
   <span class="n">handle_raw_dataset</span><span class="p">(</span><span class="n">ad</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
<p>The <cite>isType(..)</cite> function on lines 5 and 8 above is an example of one-
line type checking. The one-line check replaces a larger set of PHU
header checks which would otherwise have to be used. Users benefit in
a forward-compatible way from any future improvements to the named
type, such as better checks or incorporation of new instruments and
modes, and also gain additional sophistication such as type-hierarchy
relationships which are simply not present with the legacy approach.</p>
<p>The most general of benefits to a clean type system is the ability to
assign type-specific behaviors and still provide the using programmer
with a consistent interface to the type of functionality involved.</p>
</div>
<div class="section" id="astrodata-descriptors">
<h2>4.3. Astrodata Descriptors<a class="headerlink" href="#astrodata-descriptors" title="Permalink to this headline">¶</a></h2>
<p>A descriptor is named metadata.</p>
<p>It goes without saying that our scientific datasets contain (and
require) copious metadata. Significant amounts of &#8220;information about
the information&#8221; is present along with the pixel data regarding an
observation and much of it is important to data analysis processes.
The <a class="reference external" href="/index.php/MEF">MEF</a> file structure supports such meta-data in
the header units of the primary and other extension HDUs.</p>
<p>At first blush the problem retrieving metadata consistently is that
while the values of interest are stored in some form in the headers,
the header key names do not follow consistent conventions over all.
It&#8217;s easy to assume that there is a one to one relationship between
particular metadata headers of different instrument-modes and that the
discrepancy is that the developers have merely chosen different header
key names. If that were the entire problem a table oriented approach
could be used and one could look up the proper header key name for a
particular named piece of metadata based on the type of dataset. This
particular key would be used to look up the information in the
headers.</p>
<p>However, this table driven approach is not workable because the
situation turns out to be more complex. Firstly, the units of the
given header value may be different for different instruments and
modes. A table could be expanded to have columns for the value&#8217;s
storage and return type, but expanding the table in this way would
also still not be sufficient for the general case.</p>
<p>The decisive complications that preclude a simple table look-up
approach are two, and lead us to a function-based approach. One, the
information needed to provide the named metadata is sometimes
distributed across multiple key/header values. These require
combination or computation, and the for different instruments and
modes the distribution and combination requires differ. Two, a correct
calculation of the metadata sometimes requires use of look-up tables
that must be loaded from a configuration space with instrument-
specific information, based on the dataset&#8217;s Astrodata Type.</p>
<p>For metadata which complies with the more simple expectations first
considered, widely shared descriptors for some metadata are standard
functions able to lookup the meta-data based on standard names or
using simple rules that generalize whatever variation there is in the
storage of that particular meta-data across different instruments.
While it is possible for a descriptor to store its calculated value in
the header of the dataset, and return that if called again,
essentially caching the value in the header, Gemini descriptors choose
as a matter of policy to always recalculate, and leave such caching-
schemes to the calling program.</p>
<p>A complete descriptor definition includes the proper unit for the
descriptor and a conceptual description (<a class="reference external" href="/index.php?title=Template:URL_GEMINI_DESCRIPTORS&amp;action=edit&amp;redlink=1">Template:URL GEMINI
DESCRIPTORS</a>). E.g. Any CCD based data will have an associated
&#8220;gain&#8221;, relating to the electronics used to take the image. Given an
AstroData instance, ad , to get the &#8220;gain&#8221; for any supported Astrodata
Type, you would use the following source code regardless of the
instrument-mode of the dataset:</p>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><pre>1</pre></td><td class="code"><div class="highlight"><pre> <span class="n">gain</span> <span class="o">=</span> <span class="n">ad</span><span class="o">.</span><span class="n">gain</span><span class="p">()</span>
</pre></div>
</td></tr></table></div>
<p>Because the proper descriptors are assigned to the correct Astrodata
Types for Gemini Instruments, the line above will take into account
any type-specific peculiarities that exist between any supported
dataset. The current ADCONFIG_Gemini configuration implementation has
descriptors present for all Gemini instruments. See &#8220;Gemini AstroData
Type Reference&#8221; (<a class="reference external" href="http://www.gemini.edu/INSERTFINALGATREFURLHERE">http://www.gemini.edu/INSERTFINALGATREFURLHERE</a>) for a list of
available descriptors for Gemini data. Note that descriptor names
themselves are not covered in the Astrodata Users Manual itself
because they are part of the type-specific configuration package.</p>
</div>
<div class="section" id="recipe-system-primitives">
<h2>4.4. Recipe System Primitives<a class="headerlink" href="#recipe-system-primitives" title="Permalink to this headline">¶</a></h2>
<p>A primitive is a named transformation.</p>
<p>A primitive is meant to name an abstract dataset transformation for
which we will want to assign concrete implementations on a per
Astrodata Type basis. E.g. &#8220;subtractSky&#8221; is a transformation that has
meaning for a variety of wavelength regimes which involve subtracting
sky frames from the science pixels. Nevertheless, different
instruments in different modes will require different implementations
for this transformation, due both to differences in the data type and
data layout produced by a particular instrument-mode, and also due to
different common reduction practices in different wavelength regimes.</p>
<p>Recipe and primitive names both play a role bridging the gap between
what the computer does and what the science user expects to be done,
which details to expose, e.g. in a primitive or recipe name, and which
details to obscure and assume are unimportant if done &#8220;properly for
the given type of data&#8221;. The primitives are thus meant to be human-
recognizable steps such as come up in a discussion among science users
about data flow procedures. The recipes are, loosely, the names of
data processing work, and the primitives are names for human-
recognizable steps in that process. This puts a constraint on how
functionally fine grained primitives should becomes. For example at
Gemini we have assumed the concept of primitives as &#8220;scientifically
meaningful&#8221; steps means the data should never be in an incoherent or
invalid state, scientifically, after a given step. Each step is at
least a mini-milestone of reductio, and for example no step should
require another step to complete for its own transformation to be
considered complete, such as updating pixel data without making the
corresponding header changes.</p>
<p>The fact that recipes can call recipes addresses allows refactoring
between recipes and primitives as the set of transformation evolves. A
recipe called by a higher level recipe is seen as an atomic step at
the level of the calling recipe, and satisfies the requirement. But to
experts in the mode being processed, this recipe in turn is made of
coherent steps and these steps also satisfy the requirement. Coherent
steps which can be broken down into smaller coherent steps are thus
probably best addressed with a recipe calling a recipe. This formation
helps recipes to work for more types. At bottom primitives have to be
executed so that actual python can run and manipulate the dataset, but
below a certain level of granularity primitives become inappropriate.
Such code, insofar as it is reusable and/or needs to be encapsulated,
is written as functions in utility libraries, such as the Gemini
&#8220;gempy&#8221; package.</p>
<p>Formalizing the transformation concept allows us to refactor our data
reduction approaches due to unforeseen complications, new information,
new instruments, and so on, without having to necessarily change
recipes that call these transformations, or the named transformations
which the recipes themselves represent. Recipes for specific nodes in
the Astrodata Type tree can also be assigned as needed, and the fact
that recipes and primitives can be used by name interchangeably
ensures that transformations can be refactored and solved with
different levels of recipe and primitive as experience grows and needs
evolve. This flexibility helps us expand and improve the available
transformations while still providing a stable interface to the user.</p>
<p>AstroData is intended to be useful for general python scripting, that
is, one does not have to write code in the form of primitives to use
Astrodata. And, as mentioned previously, the Recipe System is not
automatically imported (i.e. as a result of &#8220;import astrodata&#8221;) so
that no overhead is borne by the AstroData user not making use of
automation features, such as when writing a script. Further, a script
using AstroData benefits from the type, descriptor, validation, and
other built in data handling features of AstroData. However, such
scripts do not lend themselves to use in a well-controlled automated
system, and thus the Recipe System is provided for when there is need
for such a system in which to execute the transformation, as with the
Gemini Pipeline projects. Unconstrained python scripts lack a
consistent control and parameter interface.</p>
<p>When writing primitives all inputs are provided through the Reduction
Context, and depending on the control system these may come from the
unix command line, the pyraf command line, from a pipeline control
system or other software, or by the calling recipes and primitives.
Primitive functions are written as python generators, allowing the
control system to perform some tasks for the primitive, such as
history keeping and logging, keeping lists of stackable images,
retrieving appropriate calibrations, and reporting image statistics to
a central database, etc., when the primitive &#8220;yields&#8221;.</p>
<p>The automation system is designed to support a range of automation,
from a &#8220;dataset by dataset, fully automated&#8221; mode for pipeline
processing of data as it comes from the telescope, through to
&#8220;interactive automation&#8221; where the user decides at what level to
initiate automation and where to intervene.</p>
<p>As users advance it may be of interest to know that primitives,
strictly speaking, transform the&#8221;Reduction Context&#8221; object and not
specifically (or merely) the input datasets. This context contains
references to all objects and datasets which are part of the
reduction, including the input datasets. It is the Reduction Context
as a whole that is passed into the primitives as the standard and sole
argument for the primitive, and which must be left in a coherent state
upon final exit. For example, a primitive to calculate &#8220;seeing
quality&#8221; will not actually modify the dataset, but it will in fact
modify the Reduction Context by reporting the calculated statistic to
the reduction context via the ReductionContext class&#8217; API.</p>
<p>Below is a prototype recipe in use in our development environment for
testing. It performs some initial processing on RAW data.</p>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><pre>1
2
3
4
5
6</pre></td><td class="code"><div class="highlight"><pre><span class="n">prepare</span>
<span class="n">overscanSub</span>
<span class="n">overscanTrim</span>
<span class="n">biasSub</span>
<span class="n">flatField</span>
<span class="n">findshiftsAndCombine</span>
</pre></div>
</td></tr></table></div>
<p>Presume the above is a generic recipe. This means, given that
primitive sets for GMOS_IMAGE, NIRI_IMAGE, etc, implement the named
primitives in the recipe, then when the recipe system executes a line
such as biasSub , it will execute the &#8220;biasSub&#8221; member of the
appropries PrimitiveSet associate with that type. Thus, if prepare can
be implemented for both types, while biassub requires GMOS and NIRI-
specific implementations, then &#8220;prepare&#8221; can be implemented as a
shared recipe or in the GEMINI primitive set, while those that require
special implementation are implemented in the appropriate GMOS or NIRI
primitive sets within the correct part of the configuration.</p>
<div class="section" id="some-benefits-of-the-primitive-concept">
<h3>4.4.1. Some Benefits of the Primitive Concept<a class="headerlink" href="#some-benefits-of-the-primitive-concept" title="Permalink to this headline">¶</a></h3>
<p>Use of primitives instead of scripts for reduction processes has a
major side benefit besides enjoying automation features supplied by
the Recipe System. This benefit is due to the fact that the concept of
the primitive as a named transformation is bound to the spoken
language that Instrument Scientists, PIs, data analysts and the data
software group at Gemini use to discuss data flow procedures. This
crossover between terms in our formal system and in our less formal
spoken language has promoted consistency between the two. For example,
when breaking reductions down into discrete chunks which can be
implemented and shared when possible the process helps us understand
what truly differentiates different implementations of the same named
transformation. Sharing of code not only saves developers the effort
of reimplementation, but more importantly it promotes consistency and
provides locations in the system where wide ranging changes in policy
can be implemented accommodating the inevitable evolution of reduction
software.</p>
<p>In short, discussing how to break down Gemini&#8217;s classical reduction
procedures into recipes made of reusable primitives has had the effect
of clarifying our understanding of these procedures. Sometimes the
responsibilities of tasks in our legacy system had clear boundaries,
such as for gemarith , but for other tasks, such as the &#8220;prepare&#8221; task
in each instrument&#8217;s package, the boundaries of responsibility were
less clear. Adapting such transformation concepts already in our
spoken lexicon, and allegedly represented with concrete
implementations, guides us to creating a more concrete definition
prepare and for for the steps in prepare . A source of these
discrepancies is different practices in different wavelength and mode
regimes which cause different interpretations of how far data should
be reduced from teh raw state to a more general starting point
appropriate for a &#8220;typical PI&#8221;. Flexibility in the system allows
satisfaction of these special needs while developing truly shared
transformation concepts.</p>
<div class="section" id="natural-emergence-of-reusable-primitives">
<h4>4.4.1.1. Natural Emergence of Reusable Primitives<a class="headerlink" href="#natural-emergence-of-reusable-primitives" title="Permalink to this headline">¶</a></h4>
<p>Reusable code naturally emerged from the process above because the
work of isolating the steps taken in a data handling process naturally
reveals similar or identical steps shared by other processes, which
can then easily be implemented at a shared level. In practice, even if
creating a recipe which is over-all very instrument and mode-specific,
there seem to emerge general purpose steps which can be of benefit in
a toolkit of primitives. The corollary to this is that in the future
after implementing many of these reusable pieces as part of
accomplishing project-specific aims at the time, new project-specific
tasks will be able to select from and reuse them freely. However, the
original implementor saves time by being able to focus on their task
at hand, and make a primitive they hope is reusable, without focusing
on that as a requirement. This way we hope to benefit from
opportunistic sharing of code via the natural evolution of primitives.</p>
<p>The subsequent author has several options based on the needs of the
project at hand:</p>
<ol class="arabic simple">
<li>generalize the previous attempt at a general solution to leverage
the work already done</li>
<li>write a new generalization</li>
<li>write a version which is primarily designed to be useful as a
primitive in the project&#8217;s use case</li>
</ol>
<p>The design of the recipes and primitives of the Recipe System is
intended to facilitate negotiating these options in an environment
with fall-backs and which does not cement you into a particular layout
of your transformations. Option 3 is undesirable in general, given
sufficient time and an ideal understanding of the problem, but given
deadlines based on real world calendar events like instruments going
on sky, commissioning, et cetera, it is a desirable fall-back option
because in the end it allows the developer to focuss entirely on the
problem at hand if it proves hard to generalize. Option 3 interacts
with option 1 insofar as often a developer may find, when attempting
to generalize code, that one has created a modified version that does
work for the new case, but has broken the old case. The flexibility of
the Recipe System allows the developer to split the code, use it as
two different primitives each assigned to the correct type of datasets
using the type system, allowing one to work toward a project milestone
and defer more complete generalization of the primitive.</p>
<p>If time is not given specifically to solve the problem afterwards,
then at least when a third author requires the same functionality,
they then have the same options and fall-backs, with a greater
selection of potential primitives to use or generalize to suit their
own purposes, with preservation of old behavior as need be. For Gemini
primitives we are attempting to produce robust, general, primitives as
possible from the start, but this ability of the Recipe Configuration
to evolve is still a crucial aspect of the primitive system.</p>
<p>Test Case at Gemini Observatory: Refactoring Python Scripts into
Recipes and Primitives
++++++++++++++++++++++</p>
<p>We (GDPSG and DA teams) have performed the exercise of breaking down a
set of pre-existing scripts into recipes and primitives in the case of
some instrument monitoring scripts which are set up on a cron job.
Separate from the issue of the quality of the code being thus
preserved, the procedure for refactoring into the recipe/primitive
form turned out relatively easy and to involve the following:</p>
<ol class="arabic simple">
<li>Finding where (potential) milestone states of the data occur in the
script being refactored. These are places where the dataset and
headers are coherent, and any information the reduction context should
be informed of has been prepared and is available. Note, some
potential milestone states, when considered too fine grained will be
bundled together as a single transformation.</li>
<li>Naming the source code between each of these milestone states, and
identifying it&#8217;s input, output, and specific responsibilities.</li>
<li>Cutting and pasting (or re-entering) source from the script into a
primitive set class, adding adapter code which fetches or stores
information in the reduction context to and from variables the script
uses in its legacy form. The code can be largely left as is as
primitives are simply python code, so long as input/output is adapted
to the reduction context.</li>
<li>Writing a recipe is using the steps created above.</li>
</ol>
<p>Regarding the quality of the code thus being preserved, while it was
minimal upon analysis, as is often the case it had the advantage of
being deployed and functional. It is the intent of the Recipe system
to allow rapid adaptation of code into the system, as well as to
enable more intimately and well behaved transformations to be
integrated, and for there to be iterative refactoring paths from the
former to the latter.</p>
<p>The primitives in the test case were developed into a separate recipe
package (not in astrodata_Gemini/RECIPES_Gemini) which is added to the
Astrodata package&#8217;s RECIPEPATH environment variable. As a stand alone
package for a particular internal purpose it was not as important for
these primitives to follow idealized standards as it is for the
general purpose &#8220;astrodata_Gemini&#8221; package. Thus, instead of formal
analysis of the scripts and a resultant design, these primitives were
abstracted using the method above, from the ad hoc design of the
scripts that had been doing the work.</p>
<p>Even with lack of a formal structure to the refactoring, and the
devil-you-know approach to preserving the functioning of the code, the
process of adaptation to the recipe/primitive structure provides some
natural order and formalism in the process of identifying the de facto
transformations in the script. Improvement is incremental and procedes
de facto design of the script, (i.e. a potentially ad hoc, design-as-
you-go, non-design). But even in this case, at the very least, the
above analysis will lead to a sequential list of the steps in the
script. That alone is a good starting point for making a complete
replacement if that is necessary. Subsequent work on the recipes and
component primitives only improves the exposure of the work, the
consciousness of the ordering of operations, and merging of common
functionality into common code.</p>
<p>In the case of our instrument monitoring example the result of the
refactoring to the Recipe System is functional and in use. The
resulting recipes made use of some primitives from the Gemini library
of primitives, and could benef it from more refactoring allowing both
some primitives from the main package to be used (i.e. the scripts
performed, and primitives were adapted around a custom &#8220;prepare&#8221; step
on GMOS data), and also to allow several of the primitives created to
be made more robust and moved into the main package.</p>
</div>
</div>
<div class="section" id="recipes-calling-recipes">
<h3>4.4.2. Recipes calling Recipes<a class="headerlink" href="#recipes-calling-recipes" title="Permalink to this headline">¶</a></h3>
<p>Recipes can in fact call other recipes as well as primitives.
Primitives, also, can call recipes and other primitives. During
execution the Astrodata Recipe System makes little distinction between
recipes and primitives and from the view of those invoking recipes and
primitives, recipe and primitive names are interchangeable. E.g. a
user executing recipes through the reduce command line program can
just as easilly give a primitive name to the &#8220;reduce&#8221; command as a
&#8220;recipe&#8221; name, and reduce will execute the primitive correctly. Still
the general picture we tend to speak of is one in which we have a top
level recipe for standard processes such as making a processed bias,
which list the steps that the data must go through to complete the
processing named by the recipe. In principle these steps are
implemented in python and different types will be associated to
different implementation, but again, in reality, the recipe may be
calling other recipes which are broken down further into steps of
either sub-recipes or final primitives.</p>
<p>It is a judgment call how fine grained the list of step in a recipe
should be, and this in principle drives how fine grain primitives
should be. However, what is appropriate to view in a recipe of a
certain name and scope may not be the same granularity level which is
appropriate for specialists in the data regime being processed, as the
recipe will in general be associated with some general purpose
concepts, and should have meaning for someone with general purpose
knowledge. Sometimes if the top level recipe were to name every step
which an Instrument Analyst or Data Processing Developer found
distinct and &#8220;scientifically meaningful&#8221; this would lead to a too
finely grained list of steps, which would obscure the big picture of
how the transformation named is executed.</p>
<p>In this case, which is common, then the more finely grained steps
should be bundled together into recipes which appear just as a
primitive would, in a higher level recipe. The ability for recipes to
call recipes ensures steps can be named whatever is semantically
appropriate for whatever the scope of the transformation named might
be. At one extreme the recipe system can support a processing paradigm
in pipelines which invokes reduction with the most general
instructions, &#8220;do the appropriate thing for the next file&#8221;, and at the
other extreme it allows users to decide what to treat as atomic
processes and when to intervene.</p>
<p>The fact that primitives (should) always leave datasets at some
milestone of processing provides some security for the user that they
will not perform an operation that puts the dataset in an incoherent
state. Breaking down recipes into sub-recipes and so on into
primitives truncates at the lowest level when we have primitives that,
however focused, modify the data (or reduction context) in some
significant way and leave the dataset at some milestone of reduction,
however minor a &#8220;milestone&#8221; it may be. It&#8217;s also possible, especially
if a primitive is adapted from a script, that a primitive will be
monolithic, and cannot be broken down into a recipe until more finely
grained primitives are created. The interchangeability of recipes and
primitive names is meant to encourage such refactoring, as any
reusable set of primitives is considered more useful than a monolithic
primitive performing all the functions of the reusable set at once.</p>
</div>
</div>
<div class="section" id="astrodata-lexicon">
<h2>4.5. AstroData Lexicon<a class="headerlink" href="#astrodata-lexicon" title="Permalink to this headline">¶</a></h2>
<p>A lexicon is a list of words, and this is what the designer of an
Astrodata configuration creates. The set of terms adhere to a grammar
(types of elements that can be defined) and establishes a vocabulary
about dataset types, metadata, and transformations. Firstly, the
configurations define string type names, and criteria by which they
can be identified as a given type of dataset. Then they construct
names for and describe metadata one expects to be associated with
these datasets. Finally they create names for and describe
transformations that can be performed on datasets.</p>
<p>Datasets of particular Astrodata Types, sufficiently defined, can thus
be recognized by astrodata and the other type-specific behaviors can
be assigned. For example, the &#8220;astrodata_Gemini&#8221; package is the public
configuration package defining data taken by Gemini instruments.
Descriptors for all instruments have been created, and early
implementations of primitives for GMOS_IMAGE and GMOS are available
(and under continued development).</p>
<p>For complete documentation of the ADCONFIG_Gemini type and descriptor
package see {{GATREFNAME}, available at
<a class="reference external" href="http://www.gemini.edu/INSERTFINALGATREFURLHERE">http://www.gemini.edu/INSERTFINALGATREFURLHERE</a>.</p>
<p>The astrodata package itself has no built-in type or descriptor
definitions. It contains only the infrastructure to load such
definitions from an astrodata configuration package directory (the
path of which must appear in the PYTHONPATH, RECIPEPATH, or
ADCONFIGPATH environment variables as a directory complying with the
&#8220;astrodata_xxx&#8221; naming convention, and containing at least one of
either ADCONFIG_&lt;whatever&gt; or RECIPES_&lt;whatever&gt; sub-package.</p>
<p>Here is an part of the Gemini type hierarchy, the GMOS_IMAGE branch of
the GMOS types:</p>
<p>&lt;img alt=&#8221;GMOS AstroData Type Tree&#8221; style=&#8221;margin:.5em;padding:.5em;
border:1px black solid&#8221; width = &#8220;90%&#8221;
src=&#8221;<a class="reference external" href="http://ophiuchus.hi.gemini.edu/ADTYPETREEIMAGES/GMOS_IMAGE-tree-pd.png">http://ophiuchus.hi.gemini.edu/ADTYPETREEIMAGES/GMOS_IMAGE-tree-
pd.png</a>&#8220;/&gt;</p>
<p>astrodata package using the type definitions and graphviz, please
forgive its purely utilitarian nature.</p>
<p>This diagram shows GMOS_IMAGE is a child type of the GMOS type, which
in turn is a child of the GEMINI type. The children of GMOS_IMAGE are
other types which share some or all common primitives or other
properties with GMOS_IMAGE, but which may in some cases require
special handling. The diagram shows descriptor calculator and
primitive set assignments. A descriptor calculator (a set of
descriptor functions) is assigned to GMOS, from which GMOS_IMAGE and
GMOS_SPECT inherit the same descriptors as there is nothing more
specific assigned.</p>
<p>The graph also shows primitive sets assigned to GEMINI, GMOS, and
GMOS_IMAGE. Since a primitive set specific to GMOS_IMAGE is present in
the configuration, it would be used for transformations applying to
GMOS_IMAGE datasets rather than the GMOS or GEMINI primitives. However
the primitive set class for GMOS_IMAGE happens to be defined in
astrodata_Gemini as a child class of the GMOS primitive set, and the
GMOS primitive set as the child of the GEMINI primitive set, so in
fact, the members can be shared unless intentionally overridden.</p>
<p>Primitives associated with the GEMINI Astrodata Type are generally
just bookkeeping functions which rely on features of the Recipe System
as few pixel transformations can be entirely generalized across all
Gemini datasets, though some can. In the future, some of these
primitives will be moved to a very general type associated with any
MEF for which a more specific type is not recognized.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="creatingAPrimitive.html" title="<no title>"
             >previous</a> |</li>
        <li><a href="index.html">astrodata documentation v1.00 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2010, Craig Allen.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0b2.
    </div>
  </body>
</html>