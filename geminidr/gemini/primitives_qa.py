import astrodata
import gemini_instruments
import math
import sys
from copy import deepcopy

import numpy as np
from .lookups import qaConstraints as qa
from astropy.stats import sigma_clip
from scipy.special import j1

from gemini_instruments.gmos.pixel_functions import get_bias_level
from gempy.gemini import gemini_tools as gt
from gempy.utils import logutils
from geminidr import PrimitivesBASE
from geminidr.gemini.lookups import DQ_definitions as DQ
from .parameters_qa import ParametersQA

from recipe_system.utils.decorators import parameter_override
# ------------------------------------------------------------------------------
@parameter_override
class QA(PrimitivesBASE):
    """
    This is the class containing the QA primitives.
    """
    tagset = set(["GEMINI"])

    def __init__(self, adinputs, context, ucals=None, uparms=None):
        super(QA, self).__init__(adinputs, context, ucals=ucals,
                                         uparms=uparms)
        self.parameters = ParametersQA

    def measureBG(self, adinputs=None, stream='main', **params):
        """
        This primitive measures the sky background level for an image by
        averaging (clipped mean) the sky background level for each object 
        in the OBJCAT generated by detect_sources, ie the BACKGROUND value 
        from sextractor.

        The count levels are then converted to a flux using the nominal
        (*not* measured) Zeropoint values - the point being you want to measure
        the actual background level, not the flux incident on the top of the 
        cloud layer necessary to produce that flux level.

        Parameters
        ----------
        suffix: str
            suffix to be added to output files
        remove_bias: bool
            remove the bias level (if present) before measuring background?
        separate_ext: bool
            report one value per extension, instead of a global value?
        """
        log = self.log
        log.debug(gt.log_message("primitive", self.myself(), "starting"))
        timestamp_key = self.timestamp_keys[self.myself()]
        pars = getattr(self.parameters, self.myself())
        separate_ext = pars['separate_ext']
        remove_bias = pars['remove_bias']

        # Define a few useful numbers for formatting output
        llen = 23
        rlen = 24
        dlen = llen + rlen

        for ad in adinputs:
            # First check if the bias level has already been subtracted
            if remove_bias:
                if (ad.phu.get('BIASIM') or ad.phu.get('DARKIM') or
                any(v is not None for v in ad.hdr.get('OVERSCAN'))):
                    bias_level = None
                else:
                    try:
                        # Get the bias level
                        bias_level = get_bias_level(adinput=ad,
                                                        estimate=False)
                    except NotImplementedError:
                        # This except is here until there is a type or somesuch
                        # to test whether it is a appropriate to call the
                        # function in the try clause. Possibly replace these
                        # checks with a function that can be a bit more
                        # observing band or instrument specific
                        # MS - 2014-05-18
                        log.fullinfo(sys.exc_info()[1])
                        bias_level = None

                    if bias_level is None:
                        log.warning("Bias level not found for {}; "
                                    "approximate bias will not be removed "
                                    "from the sky level".format(ad.filename))
            else:
                bias_level = None

            # Get the filter name and the corresponding BG band definition
            # and the requested band
            filter = ad.filter_name(pretty=True)
            if filter in ['k(short)', 'kshort', 'K(short)', 'Kshort']:
                filter = 'Ks'
            try:
                bg_band_limits = qa.bgBands[filter]
            except KeyError:
                bg_band_limits = None

            req_bg = ad.requested_bg()
            pixscale = ad.pixel_scale()

            # Our preferred method is to get BG values from the OBJCAT
            bg_list = gt.measure_bg_from_objcat(ad)
            all_bg_am = []
            all_std_am = []
            info_list = []
            for index, ext in enumerate(ad):
                extver = ext.hdr.EXTVER
                ext_info = {}
                bunit = ext.hdr.get('BUNIT', 'adu')

                if bg_list[index][0] is None:
                    log.fullinfo("No good background values in {}[OBJCAT,{}],"
                        " taking median of data instead.".format(ad.filename,
                                                                 extver))
                    bg, bg_std = gt.measure_bg_from_image(ad, extver)
                    bg_list[index] = (bg, bg_std, None)

                sci_bg, sci_std, nsamples = bg_list[index]
                log.fullinfo("Raw BG level = {:.3f}".format(sci_bg))

                # Subtract bias level from BG number
                if bias_level is not None:
                    sci_bg -= bias_level[index]
                    log.fullinfo("Bias-subtracted BG level = {:.3f}".
                                 format(sci_bg))
                    bg_list[index] = (sci_bg, sci_std, nsamples)

                # Write sky background to science header
                ext.hdr.set("SKYLEVEL", sci_bg, comment="{} [{}]".
                            format(self.keyword_comments["SKYLEVEL"], bunit))

                # Get zeropoint (it's in ADU or electrons according to sciext)
                npz = ext.nominal_photometric_zeropoint()
                if npz is not None:
                    # Make sure we have a number in electrons
                    if bunit == "adu":
                        gain = ext.gain()
                        bg_e = sci_bg * gain
                        std_e = sci_std * gain
                        npz = npz + 2.5*math.log10(gain)
                    else:
                        bg_e = sci_bg
                        std_e = sci_std
                    log.fullinfo("BG electrons = {:.3f}".format(bg_e))

                    # Now divide it by the exposure time and pixel area
                    bg_e /= ext.exposure_time()*pixscale*pixscale
                    std_e /= ext.exposure_time()*pixscale*pixscale
                    log.fullinfo("BG electrons/s/as^2 = {:.3f}".format(bg_e))

                    # Now get that in (instrumental) magnitudes...
                    if bg_e<=0:
                        log.warning(
                            "Background in electrons is less than or equal "
                            "to 0 for {}:{}".format(ad.filename,extver))
                        bg_am = None
                    else:
                        bg_im = -2.5 * math.log10(bg_e)
                        log.fullinfo("BG inst mag = {:.3f}".format(bg_im))

                        # And convert to apparent magnitude using the
                        # nominal zeropoint
                        bg_am = bg_im + npz
                        log.fullinfo("BG mag = {:.3f}".format(bg_am))

                        # Error in magnitude
                        # dm = df * (2.5/ln(10)) / f 
                        std_am = std_e * (2.5/math.log(10)) / bg_e;

                        ext_info.update({"mag": bg_am, "mag_std": std_am,
                                    "electrons":bg_e, "electrons_std":std_e,
                                    "nsamples":nsamples})
                else:
                    log.stdinfo("No nominal photometric zeropoint avaliable "
                                 "for {}:{}, filter {}".format(ad.filename,
                                        extver, ad.filter_name(pretty=True)))
                    bg_am = None
                    std_am = None

                # Keep the individual values
                if bg_am is not None:
                    all_bg_am.append(bg_am)
                    all_std_am.append(std_am)
            
                bg_num = None
                bg_str = "(BG band could not be determined)"
                if bg_am is not None:
                    # Get percentile corresponding to this number
                    if separate_ext:
                        use_bg = bg_am
                    else:
                        use_bg = np.mean(all_bg_am)
                    bg_str = "BG band:".ljust(llen)
                    if bg_band_limits is not None:
                        bg20 = bg_band_limits[20]
                        bg50 = bg_band_limits[50]
                        bg80 = bg_band_limits[80]
                        if (use_bg > bg20):
                            bg_num = 20
                            bg_str += ("BG20 (>{:.2f})".
                                       format(bg20)).rjust(rlen)
                        elif (use_bg > bg50):
                            bg_num = 50
                            bg_str += ("BG50 ({:.2f}-{:.2f})".
                                       format(bg50,bg20)).rjust(rlen)
                        elif (use_bg > bg80):
                            bg_num = 80
                            bg_str += ("BG80 ({:.2f}-{:.2f})".
                                       format(bg80,bg50)).rjust(rlen)
                        else:
                            bg_num = 100
                            bg_str += ("BGAny (<{:.2f})".
                                       format(bg80)).rjust(rlen)

                # Get requested BG band
                bg_warn = ""
                if req_bg is not None:
                    if req_bg==100:                            
                        req_str = "Requested BG:".ljust(llen) + \
                                  "BGAny".rjust(rlen)
                    else:
                        req_str = "Requested BG:".ljust(llen) + \
                                  ("BG{}".format(req_bg)).rjust(rlen)
                    if bg_num is not None:
                        if req_bg<bg_num:
                            bg_warn = "\n    "+\
                                "WARNING: BG requirement not met".rjust(dlen)
                else:
                    req_str = "(Requested BG could not be determined)"

                # Log the calculated values for this extension if desired    
                if separate_ext:
                    ind = " "*logutils.SW
                    log.stdinfo(" ")
                    log.stdinfo(ind + "Filename: {}:{}".format(ad.filename,
                                                               extver))
                    log.stdinfo(ind + "-"*dlen)
                    log.stdinfo(ind + "Sky level measurement:".ljust(llen) +
                                ("{:.0f} +/- {:.0f} {}".format(sci_bg,
                                                sci_std,bunit)).rjust(rlen))
                    if bg_am is not None:
                        log.stdinfo(ind + ("Mag / sq arcsec in {}:".format(
                                    ad.filter_name(pretty=True)).ljust(llen) +
                                    ("{:.2f} +/- {:.2f}".format(bg_am,
                                                    std_am)).rjust(rlen)))
                    log.stdinfo(ind + bg_str)
                    log.stdinfo(ind + req_str+bg_warn)
                    log.stdinfo(ind + "-"*dlen)
                    log.stdinfo(" ")
                    
                # Record the band and comment in the fitsstore infodict
                if ext_info:
                    bg_comment = ["BG requirement not met"] if bg_warn else []
                    ext_info.update({"percentile_band": bg_num,
                                     "comment": bg_comment})
                info_list.append(ext_info)

            # Collapse extension-by-extension numbers
            if bg_list:
                all_bg = [v[0] for v in bg_list if v[0] is not None]
                if len(all_bg)>1:
                    all_std = np.std(all_bg)
                else:
                    all_std = [v[1] for v in bg_list
                               if v[1] is not None][0]
                all_bg = np.mean(all_bg)
            else:
                all_bg = None
            # all_bg_am is None-free
            if all_bg_am:
                if len(all_bg_am)>1:
                    all_std_am = np.std(all_bg_am)
                else:
                    all_std_am = all_std_am[0]
                all_bg_am = np.mean(all_bg_am)
            else:
                all_bg_am = None

            # Write mean background to PHU if averaging all together
            # (or if there's only one science extension)
            if (len(ad)==1 or not separate_ext) and all_bg is not None:
                ad.phu.set("SKYLEVEL", all_bg, comment="{} [{}]".
                            format(self.keyword_comments["SKYLEVEL"], bunit))

                # Log overall values if desired
                if not separate_ext:
                    ind = " "*logutils.SW
                    log.stdinfo(" ")
                    log.stdinfo(ind + "Filename: %s" % ad.filename)
                    log.stdinfo(ind + "-"*dlen)
                    log.stdinfo(ind + "Sky level measurement:".ljust(llen) +
                                ("{:.0f} +/- {:.0f} {}".format(all_bg,
                                 all_std,bunit)).rjust(rlen))
                    if all_bg_am is not None:
                        log.stdinfo(ind + ("Mag / sq arcsec in {}:".format(
                                     ad.filter_name(pretty=True))).ljust(llen) +
                                    ("{:.2f} +/- {:.2f}".format(all_bg_am,
                                                    all_std_am)).rjust(rlen))
                    log.stdinfo(ind + bg_str)
                    log.stdinfo(ind + req_str+bg_warn)
                    log.stdinfo(ind + "-"*dlen)
                    log.stdinfo(" ")

                # Report measurement to the adcc
                if all_bg_am is not None:
                    if bg_warn!="":
                        bg_comment = ["BG requirement not met"]
                    else:
                        bg_comment = []
                    qad =  {"band": bg_num,
                            "brightness": float(all_bg_am),
                            "brightness_error": float(all_std_am),
                            "requested": req_bg,
                            "comment": bg_comment,
                            }
                    #self.report_qametric(ad, "bg", qad)

            # Report measurement to fitsstore
            fitsdict = gt.fitsstore_report(ad, "sb", info_list,
                        calurl_dict=self.calurl_dict, context=self.context,
                                           upload=self.upload_metrics)

            # Timestamp and update filename
            gt.mark_history(ad, primname=self.myself(), keyword=timestamp_key)
            ad.filename = gt.filename_updater(adinput=ad, suffix=pars["suffix"],
                                              strip=True)
        return adinputs

    def measureCC(self, adinputs=None, stream='main', **params):
        """
        This primitive will determine the zeropoint by looking at sources in
        the OBJCAT for which a reference catalog magnitude has been determined

        It will also compare the measured zeropoint against the nominal
        zeropoint for the instrument and the nominal atmospheric extinction
        as a function of airmass, to compute the estimated cloud attenuation.

        This function is for use with SExtractor-style source-detection.
        It relies on having already added a reference catalog and done the
        cross match to populate the refmag column of the objcat

        The reference magnitudes (refmag) are straight from the reference
        catalog. The measured magnitudes (mags) are straight from the object
        detection catalog.

        We correct for atmospheric extinction at the point where we
        calculate the zeropoint, ie we define:
        actual_mag = zeropoint + instrumental_mag + extinction_correction

        where in this case, actual_mag is the refmag, instrumental_mag is
        the mag from the objcat, and we use the nominal extinction value as
        we don't have a measured one at this point. ie  we're actually
        computing zeropoint as:
        zeropoint = refmag - mag - nominal_extinction_correction

        Then we can treat zeropoint as: 
        zeropoint = nominal_photometric_zeropoint - cloud_extinction
        to estimate the cloud extinction.

        Parameters
        ----------
        suffix: str
            suffix to be added to output files
        """
        log = self.log
        log.debug(gt.log_message("primitive", self.myself(), "starting"))
        timestamp_key = self.timestamp_keys[self.myself()]
        pars = getattr(self.parameters, self.myself())

        # Define a few useful numbers for formatting output
        llen = 32
        rlen = 26
        dlen = llen + rlen

        # Loop over each input AstroData object in the input list
        for ad in adinputs:
            found_mag = True
            detzp_means=[]
            detzp_clouds=[]
            detzp_sigmas=[]
            total_sources=0
            qad = {}
            if not np.any(hasattr(ext, 'OBJCAT') for ext in ad):
                log.warning("No OBJCAT found in {}".format(ad.filename))
                continue

            # We really want to check for the presence of reference mags
            # in the objcats at this point, but we can more easily do a
            # quick check for the presence of reference catalogs, which are
            # a pre-requisite for this and not bother with
            # any of this if there are no reference catalogs
            if not hasattr(ad, 'REFCAT'):
                log.warning("No REFCAT present - not attempting"
                            " to measure photometric zeropoints")
                continue

            # all_ accumulate measurements through all the OBJCAT extensions
            all_cloud = []
            all_clouderr = []
            info_list = []

            nom_at_ext = ad.nominal_atmospheric_extinction()
            if nom_at_ext is None:
                log.warning("Cannot get atmospheric extinction. Assuming zero.")
                nom_at_ext = 0.0

            # Need to correct the mags for the exposure time
            exptime = ad.exposure_time()

            # If it's a funky nod-and-shuffle imaging acquistion,
            # then need to scale exposure time
            if "NODANDSHUFFLE" in ad.tags:
                log.warning("Imaging Nod-And-Shuffle. Photometry may be dubious")
                # AFAIK the number of nod_cycles isn't actually relevant -
                # there's always 2 nod positions, thus the exposure
                # time for any given star is half the total
                exptime /= 2.0
                
            for ext in ad:
                extver = ext.hdr.EXTVER
                ext_info = {}
                objcat = ext.OBJCAT
                mags = objcat["MAG_AUTO"]
                mag_errs = objcat["MAGERR_AUTO"]
                flags = objcat["FLAGS"]
                iflags = objcat["IMAFLAGS_ISO"]
                niflags = objcat["NIMAFLAGS_ISO"]
                isoarea = objcat["ISOAREA_IMAGE"]
                ids = objcat["REF_NUMBER"]
                if np.all(mags==-999):
                    log.warning("No magnitudes found in {}[OBJCAT,{}]".format(
                                ad.filename,extver))
                    continue

                magcor = 2.5*math.log10(exptime)
                mags = np.where(mags==-999, mags, mags+magcor)

                refmags = objcat["REF_MAG"]
                refmag_errs = objcat["REF_MAG_ERR"]
                if np.all(refmags==-999):
                    log.warning("No reference magnitudes found in {}[OBJCAT,{}]".
                                format(ad.filename,extver))
                    continue

                zps_type = type(refmags[0]) 

                # Calculate zeropoints for each object
                zps = refmags - mags - nom_at_ext

                # Is this mathematically correct? These are logarithmic
                # values... (PH)
                # It'll do for now as an estimate at least
                zperrs = np.sqrt((refmag_errs * refmag_errs) +
                                 (mag_errs * mag_errs))
 
                # Requirements for an object to be used
                # NaNs in zps get converted to zero here to avoid an error
                # but will be eliminated at the end
                ok = np.logical_and.reduce((np.nan_to_num(zps)>-500,
                                            flags==0, mags<90))
                if not np.all(iflags == -999):
                    # Keep objects if pristine or <2% bad/non-linear pixels
                    ok2 = np.logical_or(iflags==0,
                        np.logical_and((iflags & DQ.saturated)==0,
                                       niflags<0.02*isoarea))
                    ok = np.logical_and(ok, ok2)
                # Get rid of NaNs
                ok = np.logical_and(ok, np.logical_not(
                            np.logical_or(np.isnan(zperrs), np.isnan(zps))))
                zps = zps[ok]
                zperrs = zperrs[ok]
                ids = ids[ok]

                # Trim out where zeropoint error > err_threshold
                if len(filter(lambda z: z is not None, zps)) <= 5:
                    # 5 sources or less.  Beggars are not choosers.
                    # Raise the threshold a bit
                    ok = zperrs<0.2
                else:
                    # Use the default threshold
                    ok = zperrs<0.1
                zps = zps[ok]
                zperrs = zperrs[ok]
                ids = ids[ok]

                # OK, at this point, zps and zperrs are arrays of all
                # the zeropoints and their errors from this OBJCAT
                if len(zps)==0:
                    log.warning("No good reference sources found in {}[OBJCAT,{}]".
                                format(ad.filename,extver))
                    continue
                elif len(zps)>2:
                    # 1-sigma clip
                    m = zps.mean()
                    s = zps.std()
                    clip = (zps>m-s)&(zps<m+s)
                    zps = zps[clip]
                    zperrs = zperrs[clip]

                # If there is only one good source from which to 
                # measure the ZP, no weighting is applied
                if len(zps) == 1:
                    zp = float(zps[0])
                    zpe = float(zperrs[0])
                else:
                    # Because these are magnitude (log) values, we weight
                    # directly from the 1/variance, not signal / variance
                    weights = 1.0 / (zperrs * zperrs)

                    wzps = zps * weights
                    zp = wzps.sum() / weights.sum()
                    d = zps - zp
                    d = d*d * weights
                    zpv = d.sum() / weights.sum()
                    zpe = math.sqrt(zpv)

                # Now, in addition, we have the weighted mean zeropoint
                # and its error, from this OBJCAT in zp and zpe
                nominal_zeropoint = ext.nominal_photometric_zeropoint()
                if nominal_zeropoint is None:
                    log.warning("No nominal photometric zeropoint available "
                                "for {}:{}, filter {}".format(ad.filename,
                                 extver, ad.filter_name(pretty=True)))
                    continue

                cloud = nominal_zeropoint - zp
                clouds = nominal_zeropoint - zps
                for i in clouds:
                    all_cloud.append(i)
                for i in zperrs:
                    all_clouderr.append(i)
                detzp_means.append(zp)
                detzp_clouds.append(cloud)
                detzp_sigmas.append(zpe)
                total_sources += len(zps)
                
                # Write the zeropoint to the SCI extension header
                ext.hdr.set("MEANZP", zp, self.keyword_comments["MEANZP"])

                ind = " "*logutils.SW
                log.fullinfo("\n"+ind+"Filename: {}:{}".format(ad.filename,
                                                               extver))
                log.fullinfo(ind+"{} sources used to measure zeropoint".
                             format(len(zps)))
                log.fullinfo(ind+"-"*dlen)
                log.fullinfo(ind+"Zeropoint measurement ({} band):".format(
                             ad.filter_name(pretty=True)).ljust(llen) +
                             "{:.2f} +/- {:.2f}".format(zp, zpe).rjust(rlen))
                log.fullinfo(ind+"Nominal zeropoint:".ljust(llen) +
                             "{:.2f}".format(nominal_zeropoint).rjust(rlen))
                log.fullinfo(ind+
                             "Estimated cloud extinction:".ljust(llen) +
                             ("%.2f +/- %.2f magnitudes" % 
                             (cloud, zpe)).rjust(rlen))

                # Store the number in the QA dictionary to report to the RC
                # Ensure these are regular floats for JSON (thanks to PH)
                zp = float(zp)
                zpe = float(zpe)
                cloud = float(cloud)
                if not qad.has_key("zeropoint"):
                    qad["zeropoint"] = {}
                ampname = ext.hdr.get("AMPNAME")
                if ampname:
                    qad["zeropoint"][ampname] = {"value":zp,"error":zpe}
                else:
                    # If no ampname available, just use amp{extver}
                    # (ie. amp1, amp2...)
                    qad["zeropoint"]["amp{}".format(extver)] = {"value":zp,
                                                                "error":zpe}

                # Compose a dictionary in the format the fitsstore record wants
                # Note that mag should actually be uncorrected, and I'm not
                # sure that zpe is the right error to report here. It is a
                # little difficult to separate out the right information
                # as this primitive is currently organized
                ext_info.update({"mag":zp, "mag_std":zpe,
                                "cloud":cloud, "cloud_std":zpe,
                                "nsamples":len(zps)})
                info_list.append(ext_info)
            
            if (len(detzp_means)):
                for i in range(len(detzp_means)):
                    if i==0:
                        zp_str = "{:.2f} +/- {:.2f}".format(detzp_means[i],
                                                            detzp_sigmas[i]).rjust(rlen)
                    else:
                        zp_str += "\n   "+"{:.2f} +/- {:.2f}".format(detzp_means[i],
                                                            detzp_sigmas[i]).rjust(dlen)

                # It does not make sense to take the standard deviation
                # of a single value
                if len(all_cloud) == 1:
                    cloud = float(all_cloud[0])
                    clouderr = zpe
                else:
                    cloud = np.mean(all_cloud)
                    clouderr = np.std(all_cloud)

                # Calculate which CC band we're in. 
                # OK, the philosophy here is to do a hypothesis test for
                # each CC band. It's mathematically difficult to do a
                # hypothesis test against an arbitrary range of values,
                # So we will do one-sided tests, then walk up the scale
                # to determine the CC band.
                # To avoid having t(n-1) distributions, we assume n is large.
                # We assume that the population sigma is the sample
                # sigma+0.05mag
                pop_sigma = 0.10
                # We do a hypothesis test as follows:
                # Null hypothesis, H0: the sample is drawn from a population
                # with cloud extinction = CC_band_value
                # Alternate hypothesis, H1: the sample is drawn from a
                # population with cloud extinction > CC_band_value
                # if mean and sigma and n are those of the sample, and mu
                # is the population mean,
                # we use the test statistic = (mean - mu)/(sigma/sqrt(n))
                # Which is distributed as N(0,1) in the case of "large" n.
                # So the one-tailed critical value at the 5% level is 1.645
                # We create a dictionary: 
                #{ CCband: [mean, mu, sigma, n, 
                #           value of the test statistic, H0 is acceptable]]
                # Evaluate the test statistic for each CC band boundary,
                # with one sided tests in both directions
                cc_canbe={50: True, 70: True, 80: True, 100: True}
                H0_MSG1 = "95% confidence test indicates worse than CC{} " \
                          "(normalized test statistic {:.3f} > 1.645)"
                H0_MSG2 = "95% confidence test indicates CC{} or better " \
                          "(normalised test statistic {:.3f} < -1.645)"
                H0_MSG3 = "95% confidence test indicates borderline CC{} or one band worse " \
                          "(normalised test statistic -1.645 < {:.3f} < 1.645)"
                for cc in [50, 70, 80]:
                  ce = qa.ccBands[str(cc)]
                  ts =(cloud-ce) / ((clouderr+pop_sigma)/(math.sqrt(len(all_cloud))))
                  if(ts>1.645):
                      #H0 fails - cc is worse than the worst end of this cc band
                      log.fullinfo(H0_MSG1.format(cc, ts))
                      for c in cc_canbe.keys():
                          if(c <= cc):
                              cc_canbe[c]=False
                  if(ts<-1.645):
                      #H0 fails - cc is better than the worst end of the CC band
                      log.fullinfo(H0_MSG2.format(cc, ts))
                      for c in cc_canbe.keys():
                          if(c > cc):
                              cc_canbe[c]=False

                  if((ts<1.645) and (ts>-1.645)):
                      #H0 passes - it's consistent with the boundary
                      log.fullinfo(H0_MSG3.format(cc, ts))

                # For QA dictionary
                qad["band"] = []
                qad["comment"] = []

                ccband =[]
                l = cc_canbe.keys()
                l.sort()
                for c in l:
                    if(cc_canbe[c]):
                        qad["band"].append(c)
                        if(c==100):
                            c="Any"
                        ccband.append("CC{}".format(c))
                        # print "CC%d : %s" % (c, cc_canbe[c])
                ccband = ", ".join(ccband)

                # Get requested CC band
                cc_warn = None
                req_cc = ad.requested_cc()
                qad["requested"] = req_cc

                if req_cc is not None:
                    # Just do that one hypothesis test here
                    # Can't test for CCany, always applies
                    if(req_cc != 100):
                        try:
                            ce = qa.ccBands[str(req_cc)]
                            ts = (cloud-ce) / ((clouderr+pop_sigma)/(math.sqrt(len(all_cloud))))
                            if(ts>1.645):
                                #H0 fails - cc is worse than the worst end of this cc band
                                cc_warn = "WARNING: CC requirement not met at the 95% confidence level"
                                qad["comment"].append(cc_warn)
                        except KeyError:
                            log.warning("Requested CC value of '{}-percentile'"
                                        " NOT VALID".format(req_cc))
                            qad['comment'].append("Requested CC: '{}-percentile'"
                                        " NOT VALID".format(req_cc))

                    req_cc = "CCAny" if req_cc==100 else "CC{}".format(req_cc)
                
                ind = " " * logutils.SW
                log.stdinfo("\n"+ind+"Filename: {}".format(ad.filename))
                log.stdinfo(ind+"{} sources used to measure zeropoint".format(
                             total_sources))
                log.stdinfo(ind+"-"*dlen)
                log.stdinfo(ind+"Zeropoints by detector ({} band):".format(
                             ad.filter_name(pretty=True)).ljust(llen)+zp_str)
                log.stdinfo(ind+"Estimated cloud extinction:".ljust(llen) +
                            ("{:.2f} +/- {:.2f} magnitudes".format(cloud,
                                                        clouderr)).rjust(rlen))
                log.stdinfo(ind + "CC bands consistent with this:".ljust(llen) + 
                            ccband.rjust(rlen))
                if req_cc is not None:
                    log.stdinfo(ind+
                                "Requested CC band:".ljust(llen)+
                                req_cc.rjust(rlen))
                else:
                    log.stdinfo(ind+"(Requested CC could not be determined)")
                if cc_warn is not None:
                    log.stdinfo(ind+cc_warn)
                log.stdinfo(ind+"-"*dlen)

                # Report measurement to the adcc
                qad["extinction"] = float(cloud)
                qad["extinction_error"] = float(clouderr)
                #self.report_qametric(ad, "cc", qad)

                # Add band and comment to the info_list
                [info.update({"percentile_band": qad["band"],
                              "comment": qad["comment"]}) for info in info_list]

                # Also report to fitsstore
                fitsdict = gt.fitsstore_report(ad, "zp", info_list,
                            calurl_dict=self.calurl_dict, context=self.context,
                                               upload=self.upload_metrics)
            else:
                ind = "    "
                log.stdinfo(ind+"Filename: {}".format(ad.filename))
                log.stdinfo(ind+"Could not measure zeropoint - no catalog sources associated")

            # Timestamp and update filename
            gt.mark_history(ad, primname=self.myself(), keyword=timestamp_key)
            ad.filename = gt.filename_updater(adinput=ad, suffix=pars["suffix"],
                                              strip=True)
        return adinputs

    def measureIQ(self, adinputs=None, stream='main', **params):
        """
        This primitive is for use with sextractor-style source-detection.
        FWHM (from _profile_sources()) and CLASS_STAR (from SExtractor)
        are already in OBJCAT; this function does the clipping and reporting
        only. Measured FWHM is converted to zenith using airmass^(-0.6).

        Parameters
        ----------
        suffix: str
            suffix to be added to output files
        remove_bias: bool
            remove the bias level (if present) before measuring background?
        separate_ext: bool
            report one value per extension, instead of a global value?
        """
        log = self.log
        log.debug(gt.log_message("primitive", self.myself(), "starting"))
        timestamp_key = self.timestamp_keys[self.myself()]
        pars = getattr(self.parameters, self.myself())
        display = pars["display"]
        separate_ext = pars["separate_ext"]
        remove_bias = pars["remove_bias"]

        overlays_exist = False
        iq_overlays = []
        mean_fwhms = []
        mean_ellips = []
        for ad in adinputs:
            # Check that the data is not an image with non-square binning
            if 'IMAGE' in ad.tags:
                xbin = ad.detector_x_bin()
                ybin = ad.detector_y_bin()
                if xbin != ybin:
                    log.warning("No IQ measurement possible, image {} is {} x "
                                "{} binned data".format(ad.filename, xbin, ybin))
                    if display:
                        iq_overlays.append(None)
                    mean_fwhms.append(None)
                    mean_ellips.append(None)
                    continue

            # We may need to tile the image (and OBJCATs) so make an
            # adiq object for such purposes
            if not separate_ext and len(ad) > 1:
                adiq = deepcopy(ad)
                if remove_bias and display:
                    # Set the remove_bias parameter to False so it doesn't
                    # get removed again when display is run; leave it at
                    # default if no tiling is being done at this point,
                    # so the display will handle it later
                    remove_bias = False

                    if (ad.phu.get('BIASIM') or ad.phu.get('DARKIM') or
                        any(v is not None for v in ad.hdr.get('OVERSCAN'))):
                        log.fullinfo("Bias level has already been "
                                     "removed from data; no approximate "
                                     "correction will be performed")
                    else:
                        try:
                            # Get the bias level
                            bias_level = get_bias_level(adinput=ad,
                                                        estimate=False)
                        except NotImplementedError:
                            # This except is here until there is a type or somesuch
                            # to test whether it is a appropriate to call the
                            # function in the try clause. Possibly replace these
                            # checks with a function that can be a bit more
                            # observing band or instrument specific
                            # MS - 2014-05-18
                            log.fullinfo(sys.exc_info()[1])
                            bias_level = None

                        if bias_level is None:
                            log.warning("Bias level not found for {}; "
                                        "approximate bias will not be removed "
                                        "from the sky level".format(ad.filename))
                        else:
                            # Subtract the bias level from each extension
                            log.stdinfo("Subtracting approximate bias level "
                                        "from {} for display".
                                        format(ad.filename))
                            log.stdinfo(" ")
                            log.fullinfo("Bias levels used: {}".
                                         format(bias_level))
                            for ext, bias in zip(adiq, bias_level):
                                ext.subtract(bias)

                log.fullinfo("Tiling extensions together in order to compile "
                             "IQ data from all extensions")
                adiq = self.tileArrays([adiq], tile_all=True)[0]
            else:
                # No further manipulation, so can use a reference to the
                # original AD object instead of making a copy
                adiq = ad

            # Descriptors and other things will be the same for ad and adiq
            airmass = ad.airmass()
            wvband = ad.wavelength_band()

            # Format output for printing or logging
            llen = 32
            rlen = 24
            dlen = llen + rlen
            fnStr = "Filename: {}".format(ad.filename)

            if "IMAGE" in ad.tags:
                # Clip sources from the OBJCAT
                good_source = gt.clip_sources(adiq)
                is_image = True
            elif "SPECT" in ad.tags:
                # Fit Gaussians to the brightest continuum
                good_source = gt.fit_continuum(adiq)
                is_image = False
            else:
                log.warning("{} is not IMAGE or SPECT; no IQ measurement "
                            "will be performed".format(ad.filename))
                mean_fwhms.append(None)
                mean_ellips.append(None)
                continue

            # Check for no sources found: good_source is a list of Tables
            if all(len(t)==0 for t in good_source):
                log.warning("No good sources found in {}".format(ad.filename))
                if display:
                    iq_overlays.append(None)
                mean_fwhms.append(None)
                mean_ellips.append(None)
                continue

            # For AO observations, the AO-estimated seeing is used (the IQ
            # is also calculated from the image if possible)
            # measure Strehl if it is a NIRI or GNIRS Image
            strehl = None
            is_ao = ad.is_ao()
            if is_ao:
                wvband = "AO"
                ao_seeing = ad.ao_seeing()
                if not ao_seeing:
                    log.warning("No AO-estimated seeing found for this AO "
                                "observation")
                else:
                    log.warning("This is an AO observation, the AO-estimated "
                                "seeing will be used for the IQ band "
                                "calculation")
                if is_image and ad.instrument() in ('GSAOI', 'NIRI', 'GNIRS'):
                    if len(good_source) > 0:
                        strehl, strehl_std = _strehl(ad, good_source)
            else:
                ao_seeing = None

            info_list = []
            for src, extver in zip(good_source, adiq.hdr.EXTVER):
                if len(src) == 0:
                    log.warning("No good sources found in {}:{}".
                                format(ad.filename, extver))

                    if display:
                        iq_overlays.append(None)
                    mean_fwhms.append(None)
                    mean_ellips.append(None)
                    # If there is an AO-estimated seeing value, this can be 
                    # delivered as a metric
                    if not (is_ao and ao_seeing):
                        info_list.append({})
                        continue
                    else:
                        ell_warn = ""
                        single_warn = ""
                else:
                # Mean of clipped FWHM and ellipticity
                    # Use weights if they exist
                    if "weight" in src.columns:
                        mean_fwhm = np.average(src["fwhm_arcsec"],
                                                  weights=src["weight"])
                        std_fwhm = np.sqrt(np.average((src["fwhm_arcsec"] -
                                    mean_fwhm)**2, weights=src["weight"]))
                    else:
                        mean_fwhm = np.mean(src["fwhm_arcsec"])
                        std_fwhm = np.std(src["fwhm_arcsec"])
                    if is_image:
                        mean_ellip = np.mean(src["ellipticity"])
                        std_ellip = np.std(src["ellipticity"])
                    else:
                        mean_ellip = None
                        std_ellip = None
                    
                    # Warn if the IQ measurement is taken from a single source    
                    if len(src)==1:
                        log.warning("Only one source found. IQ numbers may "
                                    "not be accurate.")
                        single_warn = "\n    WARNING: single source IQ " + \
                        "measurement - no error available".rjust(dlen)
                    else:
                        single_warn = ""                    

                    # Warn if high ellipticity
                    if is_image and mean_ellip>0.1:
                        ell_warn = "\n    " + \
                        "WARNING: high ellipticity".rjust(dlen)
                        # Note if it is non-sidereal
                        if 'NON_SIDEREAL' in ad.tags:
                            ell_warn += ("\n     - this is likely due to "
                            "non-sidereal tracking")
                    else:
                        ell_warn = ""                    

                # Find the corrected FWHM. For AO observations, the IQ 
                # constraint band is taken from the AO-estimated seeing
                #KL TODO: This below is a messy bit of logic.  Needs to be
                #         reviewed and rethought.
                if not is_ao:
                    uncorr_iq = float(mean_fwhm)
                    uncorr_iq_std = float(std_fwhm)
                else:
                    if {'GSAOI', 'IMAGE'}.issubset(ad.tags):
                        if len(src) == 0:
                            info_list.append({})
                            continue
                        wavelength = ad.central_wavelength(asMicrometers=True)
                        magic_number = np.log10(strehl *  mean_fwhm**1.5 /
                                                wavelength**2.285)
                        # Final constant is ln(10)
                        magic_number_std = np.sqrt((strehl_std/strehl)**2 +
                             (1.5*std_fwhm/mean_fwhm)**2 + 0.15**2) / 2.3026
                        if magic_number_std == 0.0:
                            magic_number_std = 0.1
                        #log.fullinfo("MAGIC: %f %f %f %f %f" % (magic_number, magic_number_std, mean_fwhm, strehl))
                        if mean_fwhm > 0.2:
                            log.warning("Very poor image quality")
                        elif abs((magic_number + 3.00) / magic_number_std) > 3:
                            log.warning("Strehl and FWHM estimates are inconsistent")
                        # More investigation required here
                        uncorr_iq = float(7.0*mean_fwhm)
                    else:
                        uncorr_iq = ao_seeing
                    uncorr_iq_std = None
                if airmass is None:
                    log.warning("Airmass not found, not correcting to zenith")
                    corr_iq = None
                    corr_iq_std = None
                else:
                    if uncorr_iq is None:
                        log.warning("FWHM not found, not correcting to zenith")
                        corr_iq = None
                        corr_iq_std = None
                    else:
                        corr_iq = uncorr_iq * airmass**(-0.6)
                        if uncorr_iq_std is not None:
                            corr_iq_std = uncorr_iq_std * airmass**(-0.6)
                        else:
                            corr_iq_std = None
                        
                # Get IQ constraint band corresponding to the corrected FWHM 
                if corr_iq is None:
                    iq_band = None
                else:
                    iq_band = _iq_band(adinput=ad, fwhm=corr_iq)[0]

                # Format output for printing or logging
                if separate_ext:
                    fnStr += "[%s,%s]" % key
                if len(src)!=0:
                    fmStr = ("FWHM Mean +/- Sigma:").ljust(llen) + \
                            "{:.3f} +/- {:.3f} arcsec".format(mean_fwhm,
                                                        std_fwhm).rjust(rlen)
                    if is_image:
                        srcStr = "{} sources used to measure IQ.".format(len(src))
                        if('NON_SIDEREAL' in ad.tags):
                            srcStr += "\n WARNING - NON SIDEREAL tracking. IQ "
                            "measurements will be unreliable"
                        emStr = ("Ellipticity Mean +/- Sigma:").ljust(llen) + \
                                "{:.3f} +/- {:.3f}".format(mean_ellip,
                                                        std_ellip).rjust(rlen)
                    else:
                        srcStr = "IQ measured from spectra centered at rows {}".\
                            format(np.unique(src["y"]))
                if corr_iq is not None:
                    if corr_iq_std is not None:
                        csStr = "Zenith-corrected FWHM (AM {:.2f}):".format(
                            airmass).ljust(llen) + ("{:.3f} +/- {:.3f} arcsec".
                            format(corr_iq, corr_iq_std)).rjust(rlen)
                    else:
                        csStr = "Zenith-corrected FWHM (AM {:.2f}):".format(
                            airmass).ljust(llen) + ("(AO) arcsec".format(
                            corr_iq)).rjust(rlen)
                else:
                    csStr = "(Zenith FWHM could not be determined)"
                if is_ao:
                    aoStr = "AO-estimated seeing:".ljust(llen) + \
                            "{:.3f} arcsec".format(ao_seeing).rjust(rlen)
                    if strehl:
                        strehlStr = "Strehl Mean +/- Sigma:".ljust(llen) + \
                                    "{:.3f} +/- {:.3f}".format(strehl,
                                                    strehl_std).rjust(rlen)
                    else:
                        strehlStr = ("(Strehl could not be determined)")

                iq_warn = ""
                if iq_band is not None:                    
                    # iq_band is (percentile, lower bound, upper bound)
                    if iq_band[0]==20:
                        iq = "IQ20 (<{:.2f} arcsec)".format(iq_band[2])
                    elif iq_band[0]==100:
                        iq = "IQAny (>{:.2f} arcsec)".format(iq_band[1])
                    else:
                        iq = "IQ{} ({:.2f}-{:.2f} arcsec)".format(*iq_band)
                    iqStr = "IQ range for {}-band:".format(wvband).ljust(
                        llen) + iq.rjust(rlen)
                else:
                    iqStr = "(IQ band could not be determined)"
                
                # Get requested IQ band
                req_iq = ad.requested_iq()
                if req_iq is not None:
                    reqStr = "Requested IQ:".ljust(llen) + "IQ{}".format(
                        'Any' if req_iq==100 else req_iq).rjust(rlen)
                    if iq_band is not None:
                        if req_iq < iq_band[0]:
                            iq_warn = "\n    " + \
                            "WARNING: IQ requirement not met".rjust(dlen)
                else:
                    reqStr = "(Requested IQ could not be determined)"
                
                # Log final string
                ind = " " * logutils.SW
                log.stdinfo(" ")
                log.stdinfo(ind + fnStr)
                if len(src)!=0:
                    log.stdinfo(ind + srcStr)
                log.stdinfo(ind + "-"*dlen)
                if len(src) != 0:
                    log.stdinfo(ind + fmStr)
                    if is_image:
                        log.stdinfo(ind + emStr)
                        if is_ao:
                            log.stdinfo(ind + strehlStr)
                log.stdinfo(ind + csStr)
                log.stdinfo(ind + iqStr)
                log.stdinfo(ind + reqStr + ell_warn + iq_warn + single_warn)
                log.stdinfo(ind + "-"*dlen)
                log.stdinfo("")

                # Report the measurement to the adcc
                comment = []
                if iq_warn:
                    comment.append("IQ requirement not met")
                if ell_warn:
                    comment.append("High ellipticity")
                if single_warn:
                    comment.append("Single source IQ measurement, no error available")
                if len(src)!=0:
                    mean_fwhm = float(mean_fwhm)
                    std_fwhm = float(std_fwhm)
                    if not is_image:
                        comment.append("IQ measured from spectral cross-cut")
                        mean_ellip = None
                        std_ellip = None
                    else:
                        mean_ellip = float(mean_ellip)
                        std_ellip = float(std_ellip)
                    if 'NON_SIDEREAL' in ad.tags:
                        comment.append("Observation is NON SIDEREAL, IQ "
                                       "measurements will be unreliable")
                else:
                    mean_fwhm = None
                    std_fwhm = None
                    mean_ellip = None
                    std_ellip = None

                if is_ao:
                    comment.append("AO observation. IQ band from estimated AO "
                                   "seeing.")

                if iq_band is not None:
                    band = iq_band[0]
                else: 
                    band = None

                qad = {"band": band,
                       "delivered": mean_fwhm,
                       "delivered_error": std_fwhm,
                       "ellipticity": mean_ellip,
                       "ellip_error": std_ellip,
                       "zenith": corr_iq,
                       "zenith_error": corr_iq_std,
                       "is_ao": is_ao,
                       "ao_seeing": ao_seeing,
                       "strehl": strehl,
                       "requested": req_iq,
                       "comment": comment,}
                #self.report_qametric(adiq, "iq", qad)
                
                # These exist for all data (ellip=None for spectra)
                key = ('SCI', extver)
                ext_info = {"fwhm": mean_fwhm,
                                  "fwhm_std": std_fwhm,
                                  "elip": mean_ellip,
                                  "elip_std": std_ellip,
                                  "nsamples": len(src),
                                  "adaptive_optics": is_ao,
                                  "percentile_band": band,
                                  "comment": comment}
                # These only exist for images
                if is_image and len(src)>0:
                    ext_info.update({"isofwhm": np.mean(src["isofwhm_arcsec"]),
                                     "isofwhm_std": np.std(src["isofwhm_arcsec"]),
                                     "ee50d": np.mean(src["ee50d_arcsec"]),
                                     "ee50d_std": np.std(src["ee50d_arcsec"]),
                                     "pa": np.mean(src["pa"]),
                                     "pa_std": np.std(src["pa"])})
                if is_ao:
                    ext_info.update({"ao_seeing": ao_seeing, "strehl": strehl})
                
                # Store average FWHM and ellipticity, for writing
                # to output header
                mean_fwhms.append(mean_fwhm)
                mean_ellips.append(mean_ellip)

                # If displaying, make a mask to display along with image
                # that marks which stars were used
                if len(src) > 0:
                    if display:
                        if is_image:
                            data_shape = adiq.extver(extver).data.shape
                            iqmask = _iq_overlay(src, data_shape)
                            iq_overlays.append(iqmask)
                            overlays_exist = True
                        else:
                            data_shape = adiq.extver(extver).data.shape
                            iqmask = _iq_overlay(src, data_shape)
                            iq_overlays.append(iqmask)
                            overlays_exist = True
                info_list.append(ext_info)

            # Build a report to send to fitsstore
            if info_list:
                fitsdict = gt.fitsstore_report(adiq, "iq", info_list,
                        calurl_dict=self.calurl_dict, context=self.context,
                                               upload=self.upload_metrics)

            # Display image with stars used for IQ circled
            if display:
                if overlays_exist:
                    log.stdinfo("Sources used to measure IQ are marked " +
                                "with blue circles.")
                    log.stdinfo("")

                # If separate_ext is True, we want the tile parameter
                # for the display primitive to be False
                self.display([adiq], tile=not separate_ext, remove_bias=remove_bias,
                             overlay=iq_overlays)

            # Update the headers. Do this to the original AD object (ad)
            if separate_ext:
                for ext, fwhm, ellip in zip(ad, mean_fwhms, mean_ellips):
                    if fwhm is not None:
                        ext.hdr.set("MEANFWHM", fwhm,
                                comment=self.keyword_comments["MEANFWHM"])
                    if ellip is not None:
                        ext.hdr.set("MEANELLP", ellip,
                                comment=self.keyword_comments["MEANELLP"])

            if len(ad)==1 or not separate_ext:
                fwhm = mean_fwhms[0]
                ellip = mean_ellips[0]
                if fwhm is not None:
                    ad.phu.set("MEANFWHM", fwhm,
                               comment=self.keyword_comments["MEANFWHM"])
                if ellip is not None:
                    ad.phu.set("MEANELLP", ellip,
                               comment=self.keyword_comments["MEANELLP"])

            # Timestamp and update filename
            gt.mark_history(ad, primname=self.myself(), keyword=timestamp_key)
            ad.filename = gt.filename_updater(adinput=ad, suffix=pars["suffix"],
                                              strip=True)
        return adinputs

##############################################################################
# Below are the helper functions for the user level functions in this module #
##############################################################################

def _iq_band(adinput=None,fwhm=None):
    """
    Helper function to take WFS, filter, and airmass information from
    an AstroData instance and use it to convert a seeing FWHM into
    an IQ constraint band.

    Parameters
    ----------
    adinput: AD/list
        input images with measured FWHM
    fwhm: float/list
        measured FWHM (in arcseconds)

    Returns
    -------
    list: 3-tuples of (percentile, lower bound, upper bound), one per AD
    """
    iq_bands = []
    for ad, f in zip(*gt.make_lists(adinput, fwhm)):
        # The IQ bands used to depend on the WFS, but no more.
        # Now it's only the waveband that matters.  The lookup
        # table reflects this change.  Note that the lookup table
        # should be checked against the webpage from time to time
        # to ensure that they match.  ('cause they don't tell us
        # when they change it...)
        #
        # However, if AO and using AOSEEING, the WFS matters.
        if ad.is_ao():
            waveband = 'AO'
        else:
            waveband = ad.wavelength_band()

        # check that ad has valid waveband
        if waveband in qa.iqBands.keys():
            # get limits for this observation
            iq20 = qa.iqBands[waveband]["20"]
            iq70 = qa.iqBands[waveband]["70"]
            iq85 = qa.iqBands[waveband]["85"]

            # get iq band
            if f<iq20:
                iq = (20,None,iq20)
            elif f<iq70:
                iq = (70,iq20,iq70)
            elif f<iq85:
                iq = (85,iq70,iq85)
            else:
                iq = (100,iq85,None)
        else:
            iq = None

        # Append the iq band tuple to the output
        # Return value is (percentile, lower bound, upper bound)
        iq_bands.append(iq)

    return iq_bands

def _iq_overlay(stars, data_shape):
    """
    Generates a tuple of numpy arrays that can be used to mask a display with
    circles centered on the stars' positions and radii that reflect the
    measured FWHM.
    Eg. data[iqmask] = some_value

    The circle definition is based on numdisplay.overlay.circle, but circles
    are two pixels wide to make them easier to see.

    Parameters
    ----------
    stars: Table
        information (from OBJCAT) of the sources used for IQ measurement
    data_shape: 2-tuple
        shape of the data being displayed

    Returns
    -------
    tuple: arrays of x and y coordinates for overlay
    """
    xind = []
    yind = []
    width = data_shape[1]
    height = data_shape[0]
    for x0, y0 in zip(stars['x'], stars['y']):
        #radius = star["fwhm"]
        radius = 16
        r2 = radius*radius
        quarter = int(math.ceil(radius * math.sqrt (0.5)))

        for dy in range(-quarter,quarter+1):
            if r2>dy**2:
                dx = math.sqrt(r2 - dy**2)
            else:
                dx = 0
            j = int(round(dy+y0))
            i = int(round(x0-dx))           # left arc
            if i>=0 and j>=0 and i<width and j<height:
                xind.extend([i-1,i-2])
                yind.extend([j-1,j-1])
            i = int(round(x0+dx))           # right arc
            if i>=0 and j>=0 and i<width and j<height:
                xind.extend([i-1,i])
                yind.extend([j-1,j-1])

        for dx in range(-quarter, quarter+1):
            if r2>dx**2:
                dy = math.sqrt(r2 - dx**2)
            else:
                dy = 0
            i = int(round(dx + x0))
            j = int(round(y0 - dy))           # bottom arc
            if i>=0 and j>=0 and i<width and j<height:
                xind.extend([i-1,i-1])
                yind.extend([j-1,j-2])
            j = int (round (y0 + dy))           # top arc
            if i>=0 and j>=0 and i<width and j<height:
                xind.extend([i-1,i-1])
                yind.extend([j-1,j])

    iqmask = (np.array(yind),np.array(xind))
    return iqmask

def _strehl(ad, sources):
    """
    Calculate the mean Strehl ratio and its standard deviation.
    Weights are used, with brighter sources being more heavily weighted.
    This is not simply because they will have better measurements, but
    because there is a bias in SExtractor's FLUX_AUTO measurement, which
    underestimates the total flux for fainter sources (this is due to
    its extrapolation of the source profile; the apparent profile varies
    depending on how much of the uncorrected psf is detected).

    Parameters
    ----------
    ad: AstroData
        image for which we're measuring the Strehl ratio
    sources: list of Tables (one per extension)
        sources appropriate for measuring the Strehl ratio
    """
    log = logutils.get_logger(__name__)
    wavelength = ad.effective_wavelength()
    # Leave if there's no wavelength information
    if wavelength == 0.0 or wavelength is None:
        return None, None
    strehl_list = []
    strehl_weights = []
    
    for ext, src in zip(ad, sources):
        pixel_scale = ext.pixel_scale()
        for star in src:
            psf = _quick_psf(star['x'], star['y'], pixel_scale, wavelength,
                             8.1, 0.2)
            strehl = float(star['flux_max'] / star['flux'] / psf)
            #print source.x, source.y, source.flux_max, source.flux, psf, strehl
            if strehl < 0.6:
                strehl_list.append(strehl)
                strehl_weights.append(star['flux'])

    # Compute statistics with sigma-clipping and weights
    if len(strehl_list) > 0:
        data = np.array(strehl_list)
        weights = np.array(strehl_weights)
        strehl_array = sigma_clip(data)
        strehl = float(np.average(data, weights=weights))
        strehl_std = float(np.sqrt(np.average((strehl_array-strehl)**2, weights=weights)))
    else:
        strehl = None
        strehl_std = None

    return strehl, strehl_std

def _quick_psf(xc,yc, pixscale, wavelength, diameter, obsc_diam=0.0):
    """
    Calculate the peak pixel flux (normalized to total flux) for a perfect
    diffraction pattern due by a circular aperture of a given diameter
    with a central obscuration.
    
    :param xc,yc: pixel center (only subpixel location matters)
    :param pixscale: pixel scale in arcseconds
    :param wavelength: wavelength in metres
    :param diameter: diameter of aperture in metres
    :param obsc_diam: diameter of central obscuration in metres
    """
    xfrac = np.modf(float(xc))[0]
    yfrac = np.modf(float(yc))[0]
    if xfrac > 0.5:
        xfrac -= 1.0
    if yfrac > 0.5:
        yfrac -= 1.0
    # Accuracy improves with increased resolution, but subdiv=5
    # appears to give within 0.5% (always underestimated)
    subdiv = 5
    obsc = obsc_diam / diameter
    xgrid, ygrid = (np.mgrid[0:subdiv,0:subdiv]+0.5)/subdiv-0.5
    dr = np.sqrt((xfrac-xgrid)**2 + (yfrac-ygrid)**2)
    x = np.pi* diameter / wavelength * dr * pixscale / 206264.8
    sum = np.sum(np.where(x==0, 1.0, 2*(j1(x)-obsc*j1(obsc*x))/x)**2)
    sum *= (pixscale/(206264.8*subdiv) * (1-obsc*obsc))**2
    return sum / (4*(wavelength/diameter)**2/np.pi)