import sys
import math
import numpy as np
from copy import deepcopy

from astrodata.utils import Errors
from astrodata.utils import Lookups
from astrodata.utils import logutils

from gempy.gemini import gemini_tools as gt
from gempy.gemini import gemini_data_calculations as gdc

from astrodata_Gemini.ADCONFIG_Gemini.lookups import BGConstraints
from astrodata_Gemini.ADCONFIG_Gemini.lookups import CCConstraints
from astrodata_Gemini.ADCONFIG_Gemini.lookups import IQConstraints

from primitives_GENERAL import GENERALPrimitives

# ------------------------------------------------------------------------------
class QAPrimitives(GENERALPrimitives):
    """
    This is the class containing all of the primitives for the GEMINI level of
    the type hierarchy tree. It inherits all the primitives from the level
    above, 'GENERALPrimitives'.
    """
    astrotype = "GEMINI"
    
    def init(self, rc):
        GENERALPrimitives.init(self, rc)
        return rc
    init.pt_hide = True
    
    def measureBG(self, rc):
        """
        This primitive measures the sky background level for an image by
        averaging (clipped mean) the sky background level for each object 
        in the OBJCAT generated by detect_sources, ie the BACKGROUND value 
        from sextractor.

        The count levels are then converted to a flux using the nominal
        (*not* measured) Zeropoint values - the point being you want to measure
        the actual background level, not the flux incident on the top of the 
        cloud layer necessary to produce that flux level.
        """

        # Instantiate the log
        log = logutils.get_logger(__name__)

        # Log the standard "starting primitive" debug message
        log.debug(gt.log_message("primitive", "measureBG", "starting"))
        
        # Define the keyword to be used for the time stamp for this primitive
        timestamp_key = self.timestamp_keys["measureBG"]

        # Initialize the list of output AstroData objects
        adoutput_list = []

        # Get the BG band definitions from a lookup table
        bgConstraints = BGConstraints.bgConstraints

        # Define a few useful numbers for formatting output
        llen = 23
        rlen = 24
        dlen = llen + rlen

        # Loop over each input AstroData object in the input list
        for ad in rc.get_inputs_as_astrodata():
            # Get the necessary parameters from the RC
            separate_ext = rc["separate_ext"]

            # If bias is still present, it should be subtracted
            # out of the sky level number, unless user specifies not to
            remove_bias = rc["remove_bias"]
            if remove_bias:
                
                # Check whether data has been bias- or dark-subtracted
                biasim = ad.phu_get_key_value("BIASIM")
                darkim = ad.phu_get_key_value("DARKIM")

                # Check whether data has been overscan-subtracted
                overscan = np.array([ext.get_key_value("OVERSCAN") 
                                     for ext in ad["SCI"]])
                if np.any(overscan) or biasim or darkim:
                    bias_level = None
                else:
                    try:
                        # Get the bias level
                        bias_level = gdc.get_bias_level(adinput=ad,
                                                        estimate=False)
                    except NotImplementedError:
                        # This except is here until there is a type or somesuch
                        # to test whether it is a appropriate to call the
                        # function in the try clause. Possibly replace these
                        # checks with a function that can be a bit more
                        # observing band or instrument specific
                        # MS - 2014-05-18
                        log.fullinfo(sys.exc_info()[1])
                        bias_level = None

                    if bias_level is None:
                        log.warning("Bias level not found for %s; " \
                                    "approximate bias will not be removed " \
                                    "from the sky level" % ad.filename)
            else:
                bias_level = None

            # Get the filter name and the corresponding BG band definition
            # and the requested band
            filter = str(ad.filter_name(pretty=True))
            if filter in bgConstraints:
                bg_band_limits = bgConstraints[filter]
            else:
                bg_band_limits = None

            req_bg_dv = ad.requested_bg()
            if not req_bg_dv.is_none():
                req_bg = int(req_bg_dv)
            else:
                req_bg = None

            # Loop over SCI extensions
            all_bg = None
            all_std = None
            all_bg_am = None
            all_std_am = None
            bunit = None
            info_dict = {}
            for sciext in ad["SCI"]:
                extver = sciext.extver()
                objcat = ad["OBJCAT",extver]

                bunit = sciext.get_key_value("BUNIT")
                if bunit is None:
                    bunit = "adu"

                # Set nsamples=None as default, meaning median will be taken.
                # If background from sources in the catalog will be used,
                # this will be overwritten with the number of sources used
                nsamples = None

                if objcat is None:
                    log.fullinfo("No OBJCAT found for %s[SCI,%d], taking "\
                                 "median of data instead." % 
                                 (ad.filename,extver))
                    bg = None

                else:
                    bg = objcat.data["BACKGROUND"]
                    if len(bg)==0 or np.all(bg==-999):
                        log.fullinfo("No background values in %s[OBJCAT,%d], "\
                                     "taking median of data instead." %
                                     (ad.filename,extver))
                        bg = None
                    else:
                        flags = objcat.data["FLAGS"]
                        dqflag = objcat.data["IMAFLAGS_ISO"]
                        if not np.all(dqflag==-999):
                            myflags = flags | dqflag
                        else:
                            myflags = flags
                        good_bg = bg[myflags==0]

                        if len(good_bg)<3:
                            log.fullinfo("No good background values in "\
                                         "%s[OBJCAT,%d], "\
                                         "taking median of data instead." %
                                         (ad.filename,extver))
                            bg = None
                        else:

                            # sigma-clip
                            mean = np.mean(good_bg)
                            sigma = np.std(good_bg)
                            good_bg = good_bg[((good_bg < mean+sigma) & 
                                               (good_bg > mean-sigma))]
                            

                            if len(good_bg)<3:
                                log.fullinfo("No good background values in "\
                                             "%s[OBJCAT,%d], "\
                                             "taking median of data instead." %
                                             (ad.filename,extver))
                                bg = None
                            else:
                                sci_bg = np.mean(good_bg)
                                sci_std = np.std(good_bg)
                                nsamples = len(good_bg)

                if bg is None:
                    scidata = sciext.data

                    dqext = ad["DQ",extver]
                    if dqext is not None:
                        scidata = scidata[dqext.data==0]

                    if len(scidata)<2:
                        log.warning("No good values in %s[SCI,%d]" % 
                                    (ad.filename,extver))
                        continue
                    
                    # Roughly mask sources
                    median = np.median(scidata)
                    sigma = np.std(scidata)
                    scidata = scidata[scidata<median+sigma]

                    # disregard 0 (masked) values from median
                    # F2 non-illuminated values are masked as 0
                    scidata = scidata[scidata != 0]

                    sci_bg = np.median(scidata)
                    sci_std = np.std(scidata)

                log.fullinfo("Raw BG level = %f" % sci_bg)

                # Subtract bias level from BG number
                if bias_level is not None:
                    sci_bg -= bias_level[sciext.extver()]
                    log.fullinfo("Bias-subtracted BG level = %f" % sci_bg)

                # Write sky background to science header
                sciext.set_key_value(
                    "SKYLEVEL", sci_bg, comment="%s [%s]" % 
                    (self.keyword_comments["SKYLEVEL"],bunit))

                # Get nominal zeropoint
                npz = sciext.nominal_photometric_zeropoint()
                if npz._val is not None:
                    # Make sure we have a number in electrons
                    if bunit == "adu":
                        gain = float(sciext.gain())
                        bg_e = sci_bg * gain
                        std_e = sci_std * gain
                        npz = npz + 2.5*math.log10(gain)
                    else:
                        bg_e = sci_bg
                        std_e = sci_std
                    log.fullinfo("BG electrons = %f" % bg_e)

                    # Now divide it by the exposure time
                    bg_e /= float(sciext.exposure_time())
                    std_e /= float(sciext.exposure_time())
                    log.fullinfo("BG electrons/s = %f" % bg_e)

                    # Now, it's in pixels, divide it by the area of a pixel
                    # to get arcsec^2
                    pixscale = float(sciext.pixel_scale())
                    bg_e /= (pixscale*pixscale)
                    std_e /= (pixscale*pixscale)
                    log.fullinfo("BG electrons/s/as^2 = %f" % bg_e)

                    # Now get that in (instrumental) magnitudes...
                    if bg_e<=0:
                        log.warning(
                            "Background in electrons is "
                            "less than or equal to 0 for "
                            "%s[SCI,%d]" % (ad.filename,extver))
                        bg_am = None
                    else:
                        bg_im = -2.5 * math.log10(bg_e)
                        log.fullinfo("BG inst mag = %f" % bg_im)

                        # And convert to apparent magnitude using the
                        # nominal zeropoint
                        bg_am = bg_im + npz
                        log.fullinfo("BG mag = %f" % bg_am)

                        # Error in magnitude
                        # dm = df * (2.5/ln(10)) / f 
                        std_am = std_e * (2.5/math.log(10)) / bg_e;

                        info_dict[("SCI",extver)] = {"mag": bg_am,
                                                     "mag_std": std_am,
                                                     "electrons":bg_e,
                                                     "electrons_std":std_e,
                                                     "nsamples":nsamples}
                else:
                    log.stdinfo("No nominal photometric zeropoint "
                                 "available for %s[SCI,%d], filter %s" %
                                 (ad.filename,sciext.extver(),filter))
                    bg_am = None
                    std_am = None

                # Keep a running average value
                if all_bg is None:
                    all_bg = sci_bg
                    all_std = sci_std
                    if bg_am is not None:
                        all_bg_am = bg_am
                        all_std_am = std_am
                else:
                    all_bg = np.mean([all_bg,sci_bg])
                    all_std = np.sqrt(all_std**2+sci_std**2)
                    if (bg_am is not None) and (all_bg_am is not None):
                        all_bg_am = np.mean([all_bg_am,bg_am])
                        all_std_am = np.sqrt(all_std_am**2+std_am**2)
                    elif bg_am is not None:
                        all_bg_am = bg_am
                        all_std_am = std_am
            
                bg_num = None
                bg_str = "(BG band could not be determined)"
                if bg_am is not None:

                    # Get percentile corresponding to this number
                    if separate_ext:
                        use_bg = bg_am
                    else:
                        use_bg = all_bg_am
                    bg_str = "BG band:".ljust(llen)
                    if bg_band_limits is not None:
                        bg20 = bg_band_limits[20]
                        bg50 = bg_band_limits[50]
                        bg80 = bg_band_limits[80]
                        if(use_bg > bg20):
                            bg_num = 20
                            bg_str += ("BG20 (>%.2f)" % bg20).rjust(rlen)
                        elif(use_bg > bg50):
                            bg_num = 50
                            bg_str += ("BG50 (%.2f-%.2f)" % 
                                       (bg50,bg20)).rjust(rlen)
                        elif(use_bg > bg80):
                            bg_num = 80
                            bg_str += ("BG80 (%.2f-%.2f)" % 
                                       (bg80,bg50)).rjust(rlen)
                        else:
                            bg_num = 100
                            bg_str += ("BGAny (<%.2f)" % bg80).rjust(rlen)

                # Get requested BG band
                bg_warn = ""
                if req_bg is not None:
                    if req_bg==100:                            
                        req_str = "Requested BG:".ljust(llen) + \
                                  "BGAny".rjust(rlen)
                    else:
                        req_str = "Requested BG:".ljust(llen) + \
                                  ("BG%d" % req_bg).rjust(rlen)
                    if bg_num is not None:
                        if req_bg<bg_num:
                            bg_warn = "\n    "+\
                                "WARNING: BG requirement not met".rjust(dlen)
                else:
                    req_str = "(Requested BG could not be determined)"

                # Log the calculated values for this extension if desired    
                if separate_ext:
                    ind = " " * rc["logindent"]
                    log.stdinfo(" ")
                    log.stdinfo(ind + "Filename: %s[SCI,%d]" % 
                                (ad.filename,extver))
                    log.stdinfo(ind + "-"*dlen)
                    log.stdinfo(ind + "Sky level measurement:".ljust(llen) +
                                ("%.0f +/- %.0f %s" % 
                                 (sci_bg,sci_std,bunit)).rjust(rlen))
                    if bg_am is not None:
                        log.stdinfo(ind + ("Mag / sq arcsec in %s:" % 
                                     filter).ljust(llen) + 
                                    ("%.2f +/- %.2f" % 
                                     (bg_am,std_am)).rjust(rlen))
                    log.stdinfo(ind + bg_str)
                    log.stdinfo(ind + req_str+bg_warn)
                    log.stdinfo(ind + "-"*dlen)
                    log.stdinfo(" ")
                    
                # Record the band and comment in the fitsstore infodict
                if info_dict.has_key(("SCI",extver)):
                    info_dict[("SCI",extver)]["percentile_band"] = bg_num
                    if bg_warn!="":
                        bg_comment = ["BG requirement not met"]
                    else:
                        bg_comment = []
                    info_dict[("SCI",extver)]["comment"] = bg_comment

            # Write mean background to PHU if averaging all together
            # (or if there's only one science extension)
            if (ad.count_exts("SCI")==1 or not separate_ext) \
                    and all_bg is not None:
                ad.phu_set_key_value("SKYLEVEL", all_bg, comment="%s [%s]" % \
                                     (self.keyword_comments["SKYLEVEL"], bunit))

                # Log overall values if desired
                if not separate_ext:
                    if "logindent" in rc:
                        ind = " " * rc["logindent"]
                    else:
                        ind = ""
                    log.stdinfo(" ")
                    log.stdinfo(ind + "Filename: %s" % ad.filename)
                    log.stdinfo(ind + "-"*dlen)
                    log.stdinfo(ind + "Sky level measurement:".ljust(llen) +
                                ("%.0f +/- %.0f %s" % 
                                 (all_bg,all_std,bunit)).rjust(rlen))
                    if all_bg_am is not None:
                        log.stdinfo(ind + ("Mag / sq arcsec in %s:"% 
                                     filter).ljust(llen) + 
                                    ("%.2f +/- %.2f" % 
                                     (all_bg_am, all_std_am)).rjust(rlen))
                    log.stdinfo(ind + bg_str)
                    log.stdinfo(ind + req_str+bg_warn)
                    log.stdinfo(ind + "-"*dlen)
                    log.stdinfo(" ")

                # Report measurement to the adcc
                if all_bg_am is not None:
                    if bg_warn!="":
                        bg_comment = ["BG requirement not met"]
                    else:
                        bg_comment = []
                    qad =  {"band": bg_num,
                            "brightness": float(all_bg_am),
                            "brightness_error": float(all_std_am),
                            "requested": req_bg,
                            "comment": bg_comment,
                            }
                    rc.report_qametric(ad, "bg", qad)

            # Report measurement to fitsstore
            fitsdict = gt.fitsstore_report(ad,rc,"sb",info_dict)

            # Add the appropriate time stamps to the PHU
            gt.mark_history(adinput=ad, keyword=timestamp_key)

            # Change the filename
            ad.filename = gt.filename_updater(adinput=ad, suffix=rc["suffix"], 
                                              strip=True)

            # Append the output AstroData object to the list 
            # of output AstroData objects
            adoutput_list.append(ad)

        # Report the list of output AstroData objects to the reduction
        # context
        rc.report_output(adoutput_list)

        yield rc

    def measureCC(self, rc):
        """
        This primitive will determine the zeropoint by looking at
        sources in the OBJCAT for which a reference catalog magnitude
        has been determined.

        It will also compare the measured zeropoint against the nominal
        zeropoint for the instrument and the nominal atmospheric extinction
        as a function of airmass, to compute the estimated cloud attenuation.

        This function is for use with sextractor-style source-detection.
        It relies on having already added a reference catalog and done the
        cross match to populate the refmag column of the objcat

        The reference magnitudes (refmag) are straight from the reference
        catalog. The measured magnitudes (mags) are straight from the object
        detection catalog.

        We correct for astromepheric extinction at the point where we
        calculate the zeropoint, ie we define:
        actual_mag = zeropoint + instrumental_mag + extinction_correction

        where in this case, actual_mag is the refmag, instrumental_mag is
        the mag from the objcat, and we use the nominal extinction value as
        we don't have a measured one at this point. ie  we're actually
        computing zeropoint as:
        zeropoint = refmag - mag - nominal_extinction_correction

        Then we can treat zeropoint as: 
        zeropoint = nominal_photometric_zeropoint - cloud_extinction
        to estimate the cloud extinction.
        """

        # Instantiate the log
        log = logutils.get_logger(__name__)

        # Log the standard "starting primitive" debug message
        log.debug(gt.log_message("primitive", "measureCC", "starting"))
        
        # Define the keyword to be used for the time stamp for this primitive
        timestamp_key = self.timestamp_keys["measureCC"]

        # Initialize the list of output AstroData objects
        adoutput_list = []

        # Get the CC band definitions from a lookup table
        ccConstraints = CCConstraints.ccConstraints

        # Define a few useful numbers for formatting output
        llen = 32
        rlen = 26
        dlen = llen + rlen

        # Loop over each input AstroData object in the input list
        for ad in rc.get_inputs_as_astrodata():
            found_mag = True
            detzp_means=[]
            detzp_clouds=[]
            detzp_sigmas=[]
            total_sources=0
            qad = {}
            # Loop over OBJCATs extensions
            objcats = ad["OBJCAT"]
            if objcats is None:
                log.warning("No OBJCAT found in %s" % ad.filename)
                adoutput_list.append(ad)
                continue
                #raise Errors.ScienceError("No OBJCAT found in %s" % ad.filename)
            # We really want to check for the presence of reference mags
            # in the objcats at this point, but we can more easily do a
            # quick check for the presence of reference catalogs, which are
            # a pre-requisite for this and not bother with
            # any of this if there are no reference catalogs
            if ad["REFCAT"] is None:
                log.warning("No Reference Catalogs Present - not attempting"
                            " to measure photometric Zeropoints")
                # This is a bit of a hack, but it circumvents the big for loop
                objcats = []

            # all_ accumulate measurements through all the OBJCAT extensions
            all_cloud = []
            all_clouderr = []

            # To pass to fitsstore report function
            info_dict = {}

            for objcat in objcats:
                extver = objcat.extver()
                mags = objcat.data["MAG_AUTO"]
                mag_errs = objcat.data["MAGERR_AUTO"]
                flags = objcat.data["FLAGS"]
                iflags = objcat.data["IMAFLAGS_ISO"]
                isoarea = objcat.data["ISOAREA_IMAGE"]
                ids = objcat.data["NUMBER"]
                if np.all(mags==-999):
                    log.warning("No magnitudes found in %s[OBJCAT,%d]"%
                                (ad.filename,extver))
                    continue

                # Need to correct the mags for the exposure time
                et = ad.exposure_time()

                # If it's a funky nod-and-shuffle imaging acquistion,
                # then need to scale exposure time
                if "GMOS_NODANDSHUFFLE" in ad.types:
                    log.warning("Imaging Nod-And-Shuffle. Photometry may be dubious")
                    # AFAIK the number of nod_cycles isn't actually relevant -
                    # there's always 2 nod positions, thus the exposure
                    # time for any given star is half the total
                    et /= 2.0
                magcor = 2.5*math.log10(et)
                mags = np.where(mags==-999,mags,mags+magcor)

                # Need to get the nominal atmospheric extinction
                nom_at_ext = ad.nominal_atmospheric_extinction()

                refmags = objcat.data["REF_MAG"]
                refmag_errs = objcat.data["REF_MAG_ERR"]
                if np.all(refmags==-999):
                    log.warning("No reference magnitudes found in %s[OBJCAT,%d]"%
                                (ad.filename,extver))
                    continue

                zps_type = type(refmags[0]) 

                zps = refmags - mags - nom_at_ext

                # Is this mathematically correct? These are logarithmic
                # values... (PH)
                # It'll do for now as an estimate at least
                zperrs = np.sqrt((refmag_errs * refmag_errs) + (mag_errs * mag_errs))
 
                # OK, trim out bad values
                zps = np.where(isoarea >= 30, zps, None)
                zps = np.where((zps > -500), zps, None)
                zps = np.where((flags == 0), zps, None)
                zps = np.where((mags < 90), zps, None)
                zperrs = np.where(isoarea >= 30, zperrs, None)
                zperrs = np.where((zps > -500), zperrs, None)
                zperrs = np.where((flags == 0), zperrs, None)
                ids = np.where(isoarea >= 30, ids, None)
                ids = np.where((zps > -500), ids, None)
                ids = np.where((flags == 0), ids, None)
                if not np.all(iflags==-999):
                    # All DQ flags are -999 if sextractor was run without
                    # a DQ plane; don't use them in this case
                    zps = np.where((iflags == 0), zps, None)
                    zperrs = np.where((iflags == 0), zperrs, None)
                    ids = np.where((iflags == 0), ids, None)

                # Trim out where zeropoint error > err_threshold
                if len(filter(lambda z: z is not None, zps)) <= 5:
                    # 5 sources or less.  Beggars are not choosers.
                    # Raise the threshold a bit
                    err_threshold = 0.2
                else:
                    # Use the default threshold
                    err_threshold = 0.1
                zps = np.where((zperrs < err_threshold), zps, None)
                zperrs = np.where((zperrs < err_threshold), zperrs, None)
                ids = np.where((zperrs < err_threshold), ids, None)
                
                # Discard the None values we just patched in
                zps = zps[np.flatnonzero(zps)]
                # While all the elements in zps are now 'float', the
                # array dtype remains 'object'.  (Note that the type
                # is set to 'object' by np.where because of the 'None's
                # that are being put in zps.)  It is not clear why the
                # array stays of dtype 'object' once the 'None's are gone.
                # When something similar is done from the Python interactive
                # shell, once the 'None's are gone, the array dtype gets
                # automatically set to np.float64.
                # All this is important because zps.mean() will fail later
                # if zps is dtype 'object' and the elements are Python
                # 'float's.
                zps = np.asarray(zps, dtype=zps_type)
                zperrs = zperrs[np.flatnonzero(zperrs)]
                ids = ids[np.flatnonzero(ids)]

                # OK, at this point, zps and zperrs are arrays of all
                # the zeropoints and their errors from this OBJCAT
                if len(zps)==0:
                    log.warning("No good reference sources found in %s[OBJCAT,%d]"%
                                (ad.filename,extver))
                    continue
                elif len(zps)>2:
                    # 1-sigma clip
                    m = zps.mean()
                    s = zps.std()
                    clip = (zps>m-s)&(zps<m+s)
                    zps = zps[clip]
                    zperrs = zperrs[clip]

                # If there is only one good source from which to 
                # measure the ZP, no weighting is applied
                if len(zps) == 1:
                    zp = float(zps[0])
                    zpe = float(zperrs[0])
                else:
                    # Because these are magnitude (log) values, we weight
                    # directly from the 1/variance, not signal / variance
                    weights = 1.0 / (zperrs * zperrs)

                    wzps = zps * weights
                    zp = wzps.sum() / weights.sum()
                    d = zps - zp
                    d = d*d * weights
                    zpv = d.sum() / weights.sum()
                    zpe = math.sqrt(zpv)

                # Now, in addition, we have the weighted mean zeropoint
                # and its error, from this OBJCAT in zp and zpe
                nominal_zeropoint = (
                  ad["SCI", extver].nominal_photometric_zeropoint())
                if nominal_zeropoint.is_none():
                    log.warning("No nominal photometric zeropoint "\
                                "available for %s[SCI,%d], filter %s" %
                                (ad.filename,extver,
                                 ad.filter_name(pretty=True)))
                    continue

                cloud = nominal_zeropoint - zp
                clouds = nominal_zeropoint - zps
                for i in clouds:
                    all_cloud.append(i)
                for i in zperrs:
                    all_clouderr.append(i)
                detzp_means.append(zp)
                detzp_clouds.append(cloud)
                detzp_sigmas.append(zpe)
                total_sources += len(zps)
                
                # Write the zeropoint to the SCI extension header
                ad["SCI", extver].set_key_value(
                    "MEANZP", zp, comment=self.keyword_comments["MEANZP"])

                ind = " " * rc["logindent"]
                log.fullinfo("\n"+ind+"Filename: %s [\"OBJCAT\", %d]" % 
                            (ad.filename, extver))
                log.fullinfo(ind+"%d sources used to measure zeropoint" % 
                             len(zps))
                log.fullinfo(ind+"-"*dlen)
                log.fullinfo(ind+
                             ("Zeropoint measurement (%s band):" % 
                              ad.filter_name(pretty=True)).ljust(llen) +
                             ("%.2f +/- %.2f" % (zp, zpe)).rjust(rlen))
                log.fullinfo(ind+
                             ("Nominal zeropoint:").ljust(llen) +
                             ("%.2f" % nominal_zeropoint).rjust(rlen))
                log.fullinfo(ind+
                             "Estimated cloud extinction:".ljust(llen) +
                             ("%.2f +/- %.2f magnitudes" % 
                             (cloud, zpe)).rjust(rlen))

                # Store the number in the QA dictionary to report to the RC
                if not qad.has_key("zeropoint"):
                    qad["zeropoint"] = {}
                ampname = ad["SCI", extver].get_key_value("AMPNAME")
                if ampname:
                    qad["zeropoint"][ampname] = {"value":zp,"error":zpe}
                else:
                    # If no ampname available, just use amp{extver}
                    # (ie. amp1, amp2...)
                    qad["zeropoint"]["amp%d" % extver] = {"value":zp,
                                                          "error":zpe}

                # Compose a dictionary in the format the fitsstore record wants
                # Note that mag should actually be uncorrected, and I'm not
                # sure that zpe is the right error to report here. It is a
                # little difficult to separate out the right information
                # as this primitive is currently organized
                info_dict[("SCI",extver)] = {"mag":zp,
                                             "mag_std":zpe,
                                             "cloud":cloud,
                                             "cloud_std":zpe,
                                             "nsamples":len(zps),}
            
            if(len(detzp_means)):
                for i in range(len(detzp_means)):
                    if i==0:
                        zp_str = ("%.2f +/- %.2f" % 
                                 (detzp_means[i], detzp_sigmas[i])).rjust(rlen)
                    else:
                        zp_str += "\n    "
                        zp_str += ("%.2f +/- %.2f" % 
                                 (detzp_means[i], detzp_sigmas[i])).rjust(dlen)

                # It does not make sense to take the standard deviation
                # of a single value
                if len(all_cloud) == 1:
                    cloud = float(all_cloud[0])
                    clouderr = zpe
                else:
                    cloud = np.mean(all_cloud)
                    clouderr = np.std(all_cloud)

                # Calculate which CC band we're in. 
                # OK, the philosophy here is to do a hypothesis test for
                # each CC band. It's mathematically difficult to do a
                # hypothesis test against an arbitrary range of values,
                # So we will do one-sided tests, then walk up the scale
                # to determine the CC band.
                # To avoid having t(n-1) distributions, we assume n is large.
                # We assume that the population sigma is the sample
                # sigma+0.05mag
                pop_sigma = 0.10
                # We do a hypothesis test as follows:
                # Null hypothesis, H0: the sample is drawn from a population
                # with cloud extinction = CC_band_value
                # Alternate hypothesis, H1: the sample is drawn from a
                # population with cloud extinction > CC_band_value
                # if mean and sigma and n are those of the sample, and mu
                # is the population mean,
                # we use the test statistic = (mean - mu)/(sigma/sqrt(n))
                # Which is distributed as N(0,1) in the case of "large" n.
                # So the one-tailed critical value at the 5% level is 1.645
                # We create a dictionary: 
                #{ CCband: [mean, mu, sigma, n, 
                #           value of the test statistic, H0 is acceptable]]
                # Evaluate the test statistic for each CC band boundary,
                # with one sided tests in both directions
                cc_canbe={50: True, 70: True, 80: True, 100: True}
                H0_MSG1 = "95%% confidence test indicates worse than CC%s " \
                          "(normalized test statistic %.3f > 1.645)"
                H0_MSG2 = "95%% confidence test indicates CC%d or better " \
                          "(normalised test statistic %.3f < -1.645)"
                H0_MSG3 = "95%% confidence test indicates borderline CC%d or one band worse " \
                          "(normalised test statistic -1.645 < %.3f < 1.645)"
                for cc in [50, 70, 80]:
                  ce = ccConstraints[str(cc)]
                  ts =(cloud-ce) / ((clouderr+pop_sigma)/(math.sqrt(len(all_cloud))))
                  if(ts>1.645):
                      #H0 fails - cc is worse than the worst end of this cc band
                      log.fullinfo(H0_MSG1 % (cc, ts))
                      for c in cc_canbe.keys():
                          if(c <= cc):
                              cc_canbe[c]=False
                  if(ts<-1.645):
                      #H0 fails - cc is better than the worst end of the CC band
                      log.fullinfo(H0_MSG2 % (cc, ts))
                      for c in cc_canbe.keys():
                          if(c > cc):
                              cc_canbe[c]=False

                  if((ts<1.645) and (ts>-1.645)):
                      #H0 passes - it's consistent with the boundary
                      log.fullinfo(H0_MSG3 % (cc, ts))

                # For QA dictionary
                qad["band"] = []
                qad["comment"] = []

                ccband =[]
                l = cc_canbe.keys()
                l.sort()
                for c in l:
                    if(cc_canbe[c]):
                        qad["band"].append(c)
                        if(c==100):
                            c="Any"
                        ccband.append("CC%s" % c)
                        # print "CC%d : %s" % (c, cc_canbe[c])
                ccband = ", ".join(ccband)

                # Get requested CC band
                cc_warn = None
                req_cc = ad.requested_cc().as_pytype()
                qad["requested"] = req_cc

                if req_cc is not None:
                    # Just do that one hypothesis test here
                    # Can't test for CCany, always applies
                    if(req_cc != 100):
                        try:
                            ce = ccConstraints[str(req_cc)]
                            ts = (cloud-ce) / ((clouderr+pop_sigma)/(math.sqrt(len(all_cloud))))
                            if(ts>1.645):
                                #H0 fails - cc is worse than the worst end of this cc band
                                cc_warn = "WARNING: CC requirement not met at the 95% confidence level"
                                qad["comment"].append(cc_warn)
                        except KeyError:
                            log.warning("Requested CC value of '%s-percentile' NOT VALID" % \
                                        req_cc)
                            qad['comment'].append("Requested CC: '%s-percentile' NOT VALID" % \
                                                  req_cc)

                    if req_cc==100:
                        req_cc = "CCAny"
                    else:
                        req_cc = "CC%d" % req_cc
                
                ind = " " * rc["logindent"]
                log.stdinfo("\n"+ind+"Filename: %s" % ad.filename)
                log.stdinfo(ind+"%d sources used to measure zeropoint" % 
                             total_sources)
                log.stdinfo(ind+"-"*dlen)
                log.stdinfo(ind+
                            ("Zeropoints by detector (%s band):"%
                             ad.filter_name(pretty=True)).ljust(llen)+
                            zp_str)
                log.stdinfo(ind+
                             "Estimated cloud extinction:".ljust(llen) +
                            ("%.2f +/- %.2f magnitudes" % 
                             (cloud, clouderr)).rjust(rlen))
                log.stdinfo(ind + "CC bands consistent with this:".ljust(llen) + 
                            ccband.rjust(rlen))
                if req_cc is not None:
                    log.stdinfo(ind+
                                "Requested CC band:".ljust(llen)+
                                req_cc.rjust(rlen))
                else:
                    log.stdinfo(ind+"(Requested CC could not be determined)")
                if cc_warn is not None:
                    log.stdinfo(ind+cc_warn)
                log.stdinfo(ind+"-"*dlen)

                # Report measurement to the adcc
                qad["extinction"] = float(cloud)
                qad["extinction_error"] = float(clouderr)
                rc.report_qametric(ad, "cc", qad)

                # Add band and comment to the info_dict
                for key in info_dict:
                    info_dict[key]["percentile_band"] = qad["band"]
                    info_dict[key]["comment"] = qad["comment"]

                # Also report to fitsstore
                fitsdict = gt.fitsstore_report(ad,rc,"zp",info_dict)

            else:
                ind = " " * rc["logindent"]
                log.stdinfo(ind+"Filename: %s" % ad.filename)
                log.stdinfo(ind+"Could not measure zeropoint - no catalog sources associated")

            # Add the appropriate time stamps to the PHU
            gt.mark_history(adinput=ad, keyword=timestamp_key)

            # Change the filename
            ad.filename = gt.filename_updater(adinput=ad, suffix=rc["suffix"], 
                                              strip=True)

            # Append the output AstroData object to the list 
            # of output AstroData objects
            adoutput_list.append(ad)

        # Report the list of output AstroData objects to the reduction
        # context
        rc.report_output(adoutput_list)

        yield rc

    def measureIQ(self, rc):
        """
        This primitive is for use with sextractor-style source-detection.
        FWHM are already in OBJCAT; this function does the clipping and
        reporting only.
        """
        
        # Instantiate the log
        log = logutils.get_logger(__name__)
        
        # Log the standard "starting primitive" debug message
        log.debug(gt.log_message("primitive", "measureIQ", "starting"))
        
        # Define the keyword to be used for the time stamp for this primitive
        timestamp_key = self.timestamp_keys["measureIQ"]

        # Initialize the list of output AstroData objects
        adoutput_list = []
        
        # Check whether display is desired
        display = rc["display"]

        # Check whether inputs need to be tiled, ie. separate_ext=False
        # and at least one input has more than one science extension
        separate_ext = rc["separate_ext"]
        remove_bias = rc["remove_bias"]
        adinput = rc.get_inputs_as_astrodata()
        orig_input = adinput
        if not separate_ext:
            next = np.array([ad.count_exts("SCI") for ad in adinput])
            if np.any(next>1):
                # Keep a deep copy of the original, untiled input to
                # report back to RC
                orig_input = [deepcopy(ad) for ad in adinput]

                # If necessary, remove an approximate bias before tiling
                if remove_bias and display:

                    # Set the remove_bias parameter to False
                    # so it doesn't get removed again when
                    # display is run; leave it at default if no
                    # tiling is being done at this point, so the
                    # display will handle it later
                    remove_bias = False

                    new_adinput = []
                    for ad in adinput:
                        # Check whether data has been bias- or dark-subtracted
                        biasim = ad.phu_get_key_value("BIASIM")
                        darkim = ad.phu_get_key_value("DARKIM")

                        # Check whether data has been overscan-subtracted
                        overscan = np.array([ext.get_key_value("OVERSCAN") 
                                             for ext in ad["SCI"]])
                        if np.any(overscan) or biasim or darkim:
                            log.fullinfo("Bias level has already been removed "\
                                         "from data; no approximate "\
                                         "correction will be performed")
                        else:
                            try:
                                # Get the bias level
                                bias_level = gdc.get_bias_level(adinput=ad,
                                                                estimate=False)
                            except NotImplementedError:
                                # This except is here until there is a type or
                                # somesuch to test whether it is a appropriate
                                # to call the function in the try clause.
                                # Possibly replace these checks with a function
                                # that can be a bit more observing band or
                                # instrument specific MS - 2014-05-18
                                log.fullinfo(sys.exc_info()[1])
                                bias_level = None
                            
                            if bias_level is None:
                                log.warning("Bias level not found for %s; " \
                                            "approximate bias will not be "\
                                            "removed from displayed image" % 
                                            ad.filename)
                            else:
                                # Subtract the bias level from each
                                # science extension
                                log.stdinfo("Subtracting approximate bias "\
                                            "level from %s for display" \
                                            % ad.filename)
                                log.stdinfo(" ")
                                log.fullinfo("Bias levels used: %s" %
                                             str(bias_level))
                                ad = ad.sub(bias_level)

                        new_adinput.append(ad)
                    adinput = new_adinput

                log.fullinfo("Tiling extensions together in order to compile "\
                             "IQ data from all extensions")
                rc.run("tileArrays(tile_all=True)")
            
        # Loop over each input AstroData object in the input list
        overlays_exist = False
        iq_overlays = []
        mean_fwhms = []
        mean_ellips = []
        for ad in rc.get_inputs_as_astrodata():

            airmass = ad.airmass()
            wvband = ad.wavelength_band()

            # Format output for printing or logging
            llen = 32
            rlen = 24
            dlen = llen+rlen
            pm = "+/-"
            fnStr = "Filename: %s" % ad.filename

            if "IMAGE" in ad.types:
                # Clip sources from the OBJCAT
                good_source = gt.clip_sources(ad)
                is_image=True
            elif "SPECT" in ad.types:
                # Fit Gaussians to the brightest continuum
                good_source = gt.fit_continuum(ad)
                is_image=False
            else:
                log.warning("%s is not IMAGE or SPECT; no IQ "\
                            "measurement will be performed" % ad.filename)
                mean_fwhms.append(None)
                mean_ellips.append(None)
                continue

            # For AO observations, the AO-estimated seeing is used (the IQ
            # is also calculated from the image if possible)
            # measure Strehl if it is a NIRI or GNIRS Image
            strehl = None
            is_ao = ad.is_ao().as_pytype()
            if is_ao:
                ao_seeing = ad.ao_seeing().as_pytype()
                if not ao_seeing:
                    log.warning("No AO-estimated seeing found for this AO "
                                "observation")
                else:
                    log.warning("This is an AO observation, the AO-estimated "
                                "seeing will be used for the IQ band "
                                "calculation")

                ao_insts = {'GSAOI_IMAGE', 'NIRI_IMAGE', 'GNIRS_IMAGE'}
                if (typ in ao_insts for typ in ad.types):
                    strehl, strehl_std = _strehl(ad, good_source)
            else:
                ao_seeing = None

            keys = good_source.keys()

            # Check for no sources found
            if len(keys)==0:
                log.warning("No good sources found in %s" % ad.filename)
                if display:
                    iq_overlays.append(None)
                mean_fwhms.append(None)
                mean_ellips.append(None)
                continue

            # Go through all extensions
            keys.sort()
            info_dict = {}
            for key in keys:
                src = good_source[key]

                if len(src)==0:
                    if separate_ext:
                        log.warning("No good sources found in %s, "\
                                    "extension %s" %
                                    (ad.filename,key))
                    else:
                        log.warning("No good sources found in %s" %
                                    (ad.filename))

                    if display:
                        iq_overlays.append(None)
                    mean_fwhms.append(None)
                    mean_ellips.append(None)
                    # If there is an AO-estimated seeing value, this can be 
                    # delivered as a metric
                    if not (is_ao and ao_seeing):
                        continue
                    else:
                        ell_warn = ""
                else:
                # Mean of clipped FWHM and ellipticity
                    mean_fwhm = src["fwhm_arcsec"].mean()
                    std_fwhm = src["fwhm_arcsec"].std()
                    if is_image:
                        mean_ellip = src["ellipticity"].mean()
                        std_ellip = src["ellipticity"].std()
                    else:
                        mean_ellip = None
                        std_ellip = None
                    if len(src)==1:
                        log.warning("Only one source found. IQ numbers may "
                                    "not be accurate.")

                    # Warn if high ellipticity
                    if is_image and mean_ellip>0.1:
                        ell_warn = "\n    " + \
                        "WARNING: high ellipticity".rjust(dlen)
                        # Note if it is non-sidereal
                        if('NON_SIDEREAL' in ad.types):
                            ell_warn += ("\n     - this is likely due to "
                            "non-sidereal tracking")
                    else:
                        ell_warn = ""                    

                # Apply the horrible 8% sextractor -> imexam kludge
                #log.warning("Applying scale factor of 1:/1.08 to scale from "\
                #            "sextractor value to profile fit (imexam) value")
                #mean_fwhm /= 1.08
                
                # Find the corrected FWHM. For AO observations, the IQ 
                # constraint band is taken from the AO-estimated seeing
                #KL TODO: This below is a messy bit of logic.  Needs to be
                #         reviewed and rethought.
                if not is_ao:
                    uncorr_iq = float(mean_fwhm)
                    uncorr_iq_std = float(std_fwhm)
                else:
                    uncorr_iq = ao_seeing
                    uncorr_iq_std = None
                if airmass.is_none():
                    log.warning("Airmass not found, not correcting to zenith")
                    corr_iq = None
                    corr_iq_std = None
                else:
                    if uncorr_iq is None:
                        log.warning("FWHM not found, not correcting to zenith")
                        corr_iq = None
                        corr_iq_std = None
                    else:
                        corr_iq = uncorr_iq * airmass**(-0.6)
                        if uncorr_iq_std is not None:
                            corr_iq_std = uncorr_iq_std * airmass**(-0.6)
                        else:
                            corr_iq_std = None
                        
                # Get IQ constraint band corresponding to the corrected FWHM 
                if corr_iq is None:
                    iq_band = None
                else:
                    iq_band = _iq_band(adinput=ad, fwhm=corr_iq)[0]

                # Format output for printing or logging
                if separate_ext:
                    fnStr += "[%s,%s]" % key
                if len(src)!=0:
                    fmStr = ("FWHM Mean %s Sigma:" % pm).ljust(llen) + \
                            ("%.3f %s %.3f arcsec" % (mean_fwhm, pm,
                            std_fwhm)).rjust(rlen)
                    if is_image:
                        srcStr = "%d sources used to measure IQ." % len(src)
                        if('NON_SIDEREAL' in ad.types):
                            srcStr += "\n WARNING - NON SIDEREAL tracking. IQ "
                            "measurements will be unreliable"
                        emStr = ("Ellipticity Mean %s Sigma:" % pm
                                 ).ljust(llen) + ("%.3f %s %.3f" % (
                                mean_ellip, pm, std_ellip)).rjust(rlen)
                    else:
                        srcStr = "Spectrum centered at row %d used to measure " \
                                 "IQ." % p.mean(src["y"])                             
                if corr_iq is not None:
                    if corr_iq_std is not None:
                        csStr = ("Zenith-corrected FWHM (AM %.2f):" % airmass
                                 ).ljust(llen) + ("%.3f %s %.3f arcsec" % (
                                corr_iq, pm, corr_iq_std)).rjust(rlen)
                    else:
                        csStr = ("Zenith-corrected FWHM (AM %.2f):" % airmass
                                 ).ljust(llen) + ("(AO) %.3f arcsec" % corr_iq
                                ).rjust(rlen)
                        
                else:
                    csStr = "(Zenith FWHM could not be determined)"
                if is_ao:
                    aoStr = ("AO-estimated seeing:").ljust(llen) + \
                            ("%.3f arcsec" % ao_seeing).rjust(rlen)
                    if strehl:
                        strehlStr = (("Strehl (r0):").ljust(llen) +
                                     ("%.3f +/- %.3f" % (strehl,
                                      strehl_std)).rjust(rlen))

                iq_warn = ""
                if iq_band is not None:                    
                    # iq_band is (percentile, lower bound, upper bound)
                    if iq_band[0]==20:
                        iq = "IQ20 (<%.2f arcsec)" % iq_band[2]
                    elif iq_band[0]==100:
                        iq = "IQAny (>%.2f arcsec)" % iq_band[1]
                    else:
                        iq = "IQ%d (%.2f-%.2f arcsec)" % iq_band
                    iqStr = ("IQ range for %s-band:" % wvband).ljust(llen) + \
                    iq.rjust(rlen)
                else:
                    iqStr = "(IQ band could not be determined)"
                
                # Get requested IQ band
                req_iq = ad.requested_iq()
                if not req_iq.is_none():
                    if req_iq == 100:                            
                        reqStr = "Requested IQ:".ljust(llen) + \
                                 "IQAny".rjust(rlen)
                    else:
                        reqStr = "Requested IQ:".ljust(llen) + \
                                 ("IQ%d" % req_iq).rjust(rlen)
                    if iq_band is not None:
                        if req_iq < iq_band[0]:
                            iq_warn = "\n    " + \
                            "WARNING: IQ requirement not met".rjust(dlen)
                else:
                    reqStr = "(Requested IQ could not be determined)"
                
                # Log final string
                logindent = rc["logindent"]
                if logindent == None:
                    logindent = 0
                ind = " " * logindent
                log.stdinfo(" ")
                log.stdinfo(ind + fnStr)
                if len(src)!=0:
                    log.stdinfo(ind + srcStr)
                    log.stdinfo(ind + "-"*dlen)
                    log.stdinfo(ind + fmStr)
                    if is_image:
                        log.stdinfo(ind + emStr)
                log.stdinfo(ind + csStr)
                log.stdinfo(ind + iqStr)
                if is_ao and strehl:
                    log.stdinfo(ind + strehlStr)
                log.stdinfo(ind + reqStr + ell_warn + iq_warn)
                log.stdinfo(ind + "-"*dlen)
                log.stdinfo("")

                # Report the measurement to the adcc
                comment = []
                if iq_warn:
                    comment.append("IQ requirement not met")
                if ell_warn:
                    comment.append("High ellipticity")
                if len(src)!=0:
                    mean_fwhm = float(mean_fwhm)
                    std_fwhm = float(std_fwhm)
                    if not is_image:
                        comment.append("IQ measured from spectral cross-cut")
                        mean_ellip = None
                        std_ellip = None
                    else:
                        mean_ellip = float(mean_ellip)
                        std_ellip = float(std_ellip)
                    if 'NON_SIDEREAL' in ad.types:
                        comment.append("Observation is NON SIDEREAL, IQ "
                                       "measurements will be unreliable")
                else:
                    mean_fwhm = None
                    std_fwhm = None
                    mean_ellip = None
                    std_ellip = None

                if is_ao:
                    comment.append("AO observation. IQ band from estimated AO "
                                   "seeing.")

                if iq_band is not None:
                    band = iq_band[0]
                else: 
                    band = None

                qad = {"band": band,
                       "delivered": mean_fwhm,
                       "delivered_error": std_fwhm,
                       "ellipticity": mean_ellip,
                       "ellip_error": std_ellip,
                       "zenith": corr_iq,
                       "zenith_error": corr_iq_std,
                       "is_ao": is_ao,
                       "ao_seeing": ao_seeing,
                       "strehl": strehl,
                       "requested": req_iq.as_pytype(),
                       "comment": comment,}

                info_dict[key] = qad
                rc.report_qametric(ad, "iq", qad)
                
                # Store average FWHM and ellipticity, for writing
                # to output header
                mean_fwhms.append(mean_fwhm)
                mean_ellips.append(mean_ellip)

                # If displaying, make a mask to display along with image
                # that marks which stars were used
                if len(src)!=0:
                    if display:
                        if is_image:
                            data_shape=ad[key].data.shape
                            iqmask = _iq_overlay(src,data_shape)
                            iq_overlays.append(iqmask)
                            overlays_exist = True
                        else:
                            data_shape=ad[key].data.shape
                            iqmask = _iq_overlay([{"x":data_shape[1]/2,
                                               "y":np.mean(src["y"])}],data_shape)
                            iq_overlays.append(iqmask)
                            overlays_exist = True

            # Build a report to send to fitsstore
            if info_dict:
                fitsdict = gt.fitsstore_report(ad, rc, "iq", info_dict)

        # Display image with stars used circled
        if display:
            # If separate_ext is True, we want the tile parameter
            # for the display primitive to be False
            tile = str(not separate_ext)

            # Stuff overlays into RC; display primitive will look for
            # them there
            rc["overlay"] = iq_overlays
            if overlays_exist:
                log.stdinfo("Sources used to measure IQ are marked " +
                            "with blue circles.")
                log.stdinfo("")
            rc.run("display(tile=%s,remove_bias=%s)" % (tile,str(remove_bias)))

        # Update headers and filename for original input to report
        # back to RC
        for ad in orig_input:

            # Write FWHM and ellipticity to header
            if separate_ext:
                count = 0
                if len(mean_fwhms)==ad.count_exts("SCI"):
                    for sciext in ad["SCI"]:
                        mean_fwhm = mean_fwhms[count]
                        mean_ellip = mean_ellips[count]
                        if mean_fwhm is not None:
                            sciext.set_key_value("MEANFWHM", mean_fwhm,
                            comment=self.keyword_comments["MEANFWHM"])
                        if mean_ellip is not None:
                            sciext.set_key_value("MEANELLP", mean_ellip,
                            comment=self.keyword_comments["MEANELLP"])
                        count+=1
            if ad.count_exts("SCI")==1 or not separate_ext:
                mean_fwhm = mean_fwhms[0]
                mean_ellip = mean_ellips[0]
                if mean_fwhm is not None:
                    ad.phu_set_key_value("MEANFWHM", mean_fwhm,
                    comment=self.keyword_comments["MEANFWHM"])
                if mean_ellip is not None:
                    ad.phu_set_key_value("MEANELLP", mean_ellip,
                    comment=self.keyword_comments["MEANELLP"])

            # Add the appropriate time stamps to the PHU
            gt.mark_history(adinput=ad, keyword=timestamp_key)

            # Change the filename
            ad.filename = gt.filename_updater(adinput=ad, suffix=rc["suffix"], 
                                              strip=True)
                
            # Append the output AstroData object to the list 
            # of output AstroData objects
            adoutput_list.append(ad)

        # Report the list of output AstroData objects to the reduction
        # context
        rc.report_output(adoutput_list)
        
        yield rc


    def testReportQAMetric(self, rc):
        """
        This is an engineering primitive that generates test data
        for the QAP nighttime metrics GUI.
        """
        import random
        import datetime
        import time
        bigreport = {"msgtype": "qametric",
         "timestamp": time.time(),
         "iq": {"band": 85,
                "delivered": 0.983,
                "delivered_error": 0.1,
                "zenith": 0.7 + random.uniform(-.5,.5),#0.947,
                "zenith_error": 0.1,
                "ellipticity": 0.118,
                "ellip_error": 0.067,
                "requested": 85,
                "comment": ["High ellipticity"],
                },
         "cc": {"band": 70,
                "zeropoint": {"e2v 10031-23-05, right":{"value":26.80,
                                                        "error":0.05},
                              "e2v 10031-01-03, right":{"value":26.86,
                                                        "error":0.03},
                              "e2v 10031-01-03, left":{"value":26.88,
                                                       "error":0.06}},
                "extinction": .5 + random.uniform(-.5,.5),#0.02,
                "extinction_error": 0.5,
                "requested": 50,
                "comment": ["Requested CC not met"],
                },
         "bg": {"band": 100,
                "brightness": 20 + random.uniform(-.8,.8),#19.17,
                "brightness_error": 0.5,
                "requested": 100,
                "comment": []
                },
         }
        from copy import copy
        import pprint
        def mock_metadata(ad, numcall = 0):
            mtd = {"metadata":
                    { "raw_filename": ad.filename,
                      "datalabel": ad.data_label().as_pytype(),
                      "local_time": ad.local_time().as_pytype().strftime("%H:%M:%S.%f"),
                      "ut_time": ad.ut_datetime().as_pytype().strftime("%Y-%m-%d %H:%M:%S.%f"),
                      "wavelength": ad.central_wavelength(asMicrometers=True).as_pytype(),
                      "filter": ad.filter_name(pretty=True).as_pytype(),
                      "waveband": ad.wavelength_band().as_pytype(),
                      "airmass": ad.airmass().as_pytype(),
                      "instrument": ad.instrument().as_pytype(),
                      "object": ad.object().as_pytype(),
                      "wfs": ad.wavefront_sensor().as_pytype(),
                      "types": ad.types,
                    }
                  }
            import random
            now = datetime.datetime.utcnow()
            import time
            if time.timezone / 3600 ==10:
                tonight = now.replace(hour =5, minute=0)
            else:
                if now.hour>=18:
                    now += datetime.timedelta(days=1)
                    tonight = now.replace(hour =5, minute=0)
            if (not hasattr(self, "datacounter")):
                self.datacounter = 1
            nexttime = datetime.timedelta(minutes = self.datacounter*15 + random.randint(-60,0))
            self.datacounter += 1
            now_ut = tonight + nexttime

            filename = "N%sS0%0.3d.fits" % (now.strftime("%Y%m%d"),
                                            random.randint(1,999))
            dl = "GN-2012B-Q-%i-1-001" % random.randint(1,20)

            wlen = ["g","V","r","R","i","I","z","I"]
            #wlen = ["u","U","b","B","g","V","r","R","i","I","z","I","Y","Y"]
            #wlen = ["g","V"]
            wlen_ind = 2*random.randint(0,len(wlen)/2-1)
            
            mtd["metadata"].update({"ut_time": now_ut.strftime("%Y-%m-%d %H:%M:%S.%f"),
                                    "datalabel": dl,
                                    "raw_filename": filename,
                                    "filter": wlen[wlen_ind],
                                    "wavelength": wlen[wlen_ind],
                                    "waveband": wlen[wlen_ind+1]})
            delt = (now_ut - now.replace(hour=0,minute=0) )
            nowsec = float(delt.days*86400 + delt.seconds)
                
            return (mtd, nowsec)
        from time import sleep
        from math import sin
        if "test_num" in rc:
                test_num = int(rc["test_num"])
        else:
                test_num = 1
        if "test_burst" in rc:
            test_burst = int(rc["test_burst"])
        else:
                test_burst = 1
        if "test_sleep" in rc:
            test_sleep = float(rc["test_sleep"])
        else:
            test_sleep = 1.0
        for i in range(0,test_num):    
            for inp in rc.get_inputs_as_astrodata():
                mtd,nowsec = mock_metadata(inp)
                num =  sin(nowsec/60/60)*.2
                ch = random.choice(["iq", "cc", "bg"])
                if ch == "iq":
                    qad = {"band": 85,
                        "delivered": 0.983,
                        "delivered_error": 0.1,
                        "zenith": 0.7 + num*.4 ,#0.947,
                        "zenith_error": 0.1,
                        "ellipticity": 0.118,
                        "ellip_error": 0.067,
                        "requested": 85,
                        "comment": ["High ellipticity"],
                        }
                elif ch == "cc":
                    qad = {"band": 70,
                            "zeropoint": {"e2v 10031-23-05, right":{"value":26.80,
                                                                    "error":0.05},
                                          "e2v 10031-01-03, right":{"value":26.86,
                                                                    "error":0.03},
                                          "e2v 10031-01-03, left":{"value":26.88,
                                                                   "error":0.06}},
                            "extinction": .5 + num *.5,#0.02,
                            "extinction_error": 0.5,
                            "requested": 50,
                            "comment": ["Requested CC not met"],
                            }
                elif ch == "bg":
                    qad =  {"band": 100,
                            "brightness": 20 + num *.8,#19.17,
                            "brightness_error": 0.5,
                            "requested": 100,
                            "comment": []
                            }
                #print "pG108:"+pprint.pformat(qad)

                rc.report_qametric(inp, ch, qad, metadata = mtd)
            if i%test_burst == 0:
                yield rc
                sleep (test_sleep)
            yield rc


##############################################################################
# Below are the helper functions for the user level functions in this module #
##############################################################################


def _iq_band(adinput=None,fwhm=None):
    """
    Helper function to take WFS, filter, and airmass information from
    an AstroData instance and use it to convert a seeing FWHM into
    an IQ constraint band.

    :param adinput: Input images for which the FWHM has been measured
    :type adinput: Astrodata objects, either a single or a list of objects

    :param fwhm: Measured FWHM of stars in image, in arcsec
    :type fwhm: float, either a single or a list that matches the length
                of the adinput list
    """

    # Instantiate the log. This needs to be done outside of the try block,
    # since the log object is used in the except block 
    log = logutils.get_logger(__name__)

    # The validate_input function ensures that the input is not None and
    # returns a list containing one or more inputs
    adinput_list = gt.validate_input(input=adinput)
    fwhm_list = gt.validate_input(input=fwhm)

    # Initialize the list of output IQ band tuples
    list_output = []

    try:

        # get the IQ band definitions from a lookup table
        iqConstraints = IQConstraints.iqConstraints

        # if there is only one FWHM listed for all adinput, copy
        # that entry into a list that matches the length of the
        # adinput list
        if len(fwhm_list)==1:
            fwhm_list = [fwhm_list[0] for ad in adinput_list]
        else:
            # otherwise check that length of fwhm list matches 
            # the length of the adinput list
            if len(adinput_list)!=len(fwhm_list):
                raise Errors.InputError("fwhm list must match length of " +
                                        "adinput list")
        
        # Loop over each input AstroData object in the input list
        count=0
        for ad in adinput_list:

            # The IQ bands used to depend on the WFS, but no more.
            # Now it's only the waveband that matters.  The lookup
            # table reflects this change.  Note that the lookup table
            # should be checked against the webpage from time to time
            # to ensure that they match.  ('cause they don't tell us
            # when they change it...)
            #
            # However, if AO and using AOSEEING, the WFS matters.
            if ad.is_ao().as_pytype():
                waveband = 'AO'
            else:
                waveband = ad.wavelength_band().as_pytype()

            # default value for iq band
            iq = None

            # check that ad has valid waveband
            if waveband is not None and waveband in iqConstraints.keys():

                # get limits for this observation
                iq20 = iqConstraints[waveband]["20"]
                iq70 = iqConstraints[waveband]["70"]
                iq85 = iqConstraints[waveband]["85"]

                # get iq band
                if fwhm_list[count]<iq20:
                    iq = (20,None,iq20)
                    #iq="IQ20 (<%.2f arcsec)" % iq20
                elif fwhm_list[count]<iq70:
                    iq = (70,iq20,iq70)
                    #iq="IQ70 (%.2f-%.2f arcsec)" % (iq20,iq70)
                elif fwhm_list[count]<iq85:
                    iq = (85,iq70,iq85)
                    #iq="IQ85 (%.2f-%.2f arcsec)" % (iq70,iq85)
                else:
                    iq = (100,iq85,None)
                    #iq="IQAny (>%.2f arcsec)" % iq85
            
            # Append the iq band tuple to the output
            # Return value is (percentile, lower bound, upper bound)
            list_output.append(iq)
            count+=1

        # Return the list of output AstroData objects
        return list_output

    except:
        # Log the message from the exception
        log.critical(repr(sys.exc_info()[1]))
        raise

def _iq_overlay(stars,data_shape):
    """
    Generates a tuple of numpy arrays that can be used to mask a display with
    circles centered on the stars' positions and radii that reflect the
    measured FWHM.
    Eg. data[iqmask] = some_value

    The circle definition is based on numdisplay.overlay.circle, but circles
    are two pixels wide to make them easier to see.
    """

    xind = []
    yind = []
    width = data_shape[1]
    height = data_shape[0]
    for star in stars:
        x0 = star["x"]
        y0 = star["y"]
        #radius = star["fwhm"]
        radius = 16
        r2 = radius*radius
        quarter = int(math.ceil(radius * math.sqrt (0.5)))

        for dy in range(-quarter,quarter+1):
            if r2>dy**2:
                dx = math.sqrt(r2 - dy**2)
            else:
                dx = 0
            j = int(round(dy+y0))
            i = int(round(x0-dx))           # left arc
            if i>=0 and j>=0 and i<width and j<height:
                xind.extend([i-1,i-2])
                yind.extend([j-1,j-1])
            i = int(round(x0+dx))           # right arc
            if i>=0 and j>=0 and i<width and j<height:
                xind.extend([i-1,i])
                yind.extend([j-1,j-1])

        for dx in range(-quarter, quarter+1):
            if r2>dx**2:
                dy = math.sqrt(r2 - dx**2)
            else:
                dy = 0
            i = int(round(dx + x0))
            j = int(round(y0 - dy))           # bottom arc
            if i>=0 and j>=0 and i<width and j<height:
                xind.extend([i-1,i-1])
                yind.extend([j-1,j-2])
            j = int (round (y0 + dy))           # top arc
            if i>=0 and j>=0 and i<width and j<height:
                xind.extend([i-1,i-1])
                yind.extend([j-1,j])

    iqmask = (np.array(yind),np.array(xind))
    return iqmask


def _strehl(ad, sources):
    """
    Measure the Strehl Ratio (r0) on an input image.

    :param ad: astrodata image object
    :param sources: filtered sources from clip_sources
    """

    # reasonable upper strehl limit,  number of sources to use
    STREHL_LIMIT = 0.6
    RADIAN_ARCSEC = 206265
    PSF_SKY = 0.0
    PUPIL_FILE = '.strehl_pupil.npz'

    all_strehl = []
    pupil = np.array([])

    # Instantiate the log
    log = logutils.get_logger(__name__)

    # read required header values (wavelength in microns)
    n_pixels = ad.array_section().get_value()[1]
    effective_wavelength = ad.phu_get_key_value('WAVELENG') / 10000.
    inst = ad.instrument().as_pytype()
    filt = ad.filter_name(pretty=True).as_pytype()

    div_n = 1.0
    # for large arrays,  for speed approximate strehl by 'binning' pixels
    if n_pixels > 1024:
        div_n = 10.0

    n_pixels /= int(div_n)

    # phase parameters
    vv_array = np.arange(0, n_pixels, 1, dtype=float).reshape((1, n_pixels))
    uu_array = np.arange(0, n_pixels, 1, dtype=float).reshape((n_pixels, 1))
    phase_param = {'vv': _rebin(vv_array, n_pixels, n_pixels),
                   'uu': _rebin(uu_array, n_pixels, n_pixels)}

    # key for saved pupil
    pupil_key = inst + '_' + filt + '_' + str(n_pixels)

    try:
        pupil = np.load(PUPIL_FILE)[pupil_key]
    except (KeyError, IOError):
        pass

    for ext in sources:
        pixel_scale = ad[ext].pixel_scale().as_pytype()
        pixel_radians = pixel_scale / RADIAN_ARCSEC

        meter_pixel = ((effective_wavelength * 1e-6) /
                       (n_pixels * pixel_radians))

        if pupil.size == 0:
            pupil = _pupil(n_pixels, meter_pixel, inst, pupil_key, PUPIL_FILE)

        for source in sources[ext]:

            source_flx = source.flux - source.background
            source_pos = {'x': source.x / div_n - n_pixels / 2.,
                          'y': source.y / div_n - n_pixels / 2.}

            # compute perfect PSF at position of source
            psf = _perfect_psf(n_pixels, source_pos, phase_param, pupil)

            source_pos = {'x': source.x / div_n, 'y': source.y / div_n}

            psf_flx, psf_peak = _psf_phot(psf, source.flux_radius, source_pos,
                                          PSF_SKY, n_pixels)

            strehl = float((source.flux_max / source_flx) /
                           (psf_peak / psf_flx))

            if strehl <= STREHL_LIMIT:
                all_strehl.append(strehl)

    if len(all_strehl) != 0:
        strehl = np.average(all_strehl).item()
        strehl_std = np.std(all_strehl)
        log.stdinfo("Strehl (average) for %s: %s +/- %s" % (ad.filename, strehl,
                                                            strehl_std))
    else:
        strehl = None
        strehl_std = None

    return strehl, strehl_std


def _perfect_psf(n_pixels, center, phase_param, pupil):
    """
    Create an ideal psf for the Gemini telescope, to be used
    in the calculation of the Strehl Ratio r0

    :param n_pixels: number of pixels of the array
    :param center: pixel coordinate of source wrt to center
                      a psf centered on the array would be (0,0)
    :param phase_param: dictionary of phase parameters
    :param pupil: ndarray representing instrument pupil
    :param inst: instrument
    :return: the ideal psf
    """

    phase = 2. * np.pi * (phase_param['uu'] * center['x'] +
                          phase_param['vv'] * center['y']) / n_pixels

    wavefront = pupil * np.exp(1j * phase)

    # python fft requires a normalization factor of 1/N to match idl fft
    fft_2d = abs(1. / (np.size(wavefront)) * np.fft.fft2(wavefront)) ** 2

    # "shift" the array at the middle of both axes
    psf = np.roll(np.roll(fft_2d, int(n_pixels / 2), axis=1),
                  int(n_pixels / 2), axis=0)

    psf = psf / np.sum(map(np.sum, psf))

    return psf


def _pupil(n_pixels, meter_pixel, inst, pupil_key, PUPIL_FILE):
    """
    Calculates the diffraction-limited monochromatic point
    spread function (PSF) from NIRI's pupil stop, and pupil angle.
    Based on idl routines niripsf and nircs2psf

    :param n_pixels: integer 1-dimensional number of pixels
    :param meter_pixel: pixels per meter
    :param inst: string of instrument name
    :param pupil_key: dictionary key string inst+filt+npixels
    :param PUPIL_FILE: filename of file containing the saved pupils
    :return: pupil: numpy array representing pupil with psf
    """

    # Define pupil dimensions
    if inst == "GSAOI":
        PUPIL_DIMENSION = [0.2, 8.1]
    else:
        PUPIL_DIMENSION = [1.223, 7.695]

    pupil = np.zeros((n_pixels, n_pixels))
    center_point = n_pixels / 2.0
    center = {'x': center_point, 'y': center_point}

    radial_array = _dist_circle(n_pixels, center, int(n_pixels / 2))

    adjusted_array = radial_array * meter_pixel
    w = np.where((adjusted_array > PUPIL_DIMENSION[0] / 2) &
                 (adjusted_array < PUPIL_DIMENSION[1] / 2))

    if np.size(w) != 0:
        pupil[w] = 1

    _save_pupil(pupil, pupil_key, PUPIL_FILE)

    return pupil


def _rebin(a, x_new, y_new):
    """
    A python version of idl's REBIN

    :param a: input array
    :param new_shape: new square array size
    :return: reshaped array
    """

    x_orig, y_orig = a.shape

    if x_new < x_orig:
        return a.reshape((x_new, x_orig / x_new,
                          y_new, y_orig / y_new)).mean(3).mean(1)
    else:
        return np.repeat(np.repeat(a, x_new / x_orig, axis=0),
                         y_new / y_orig, axis=1)


def _dist_circle(array_size, center, radius):
    """
    Form a square array where each value is its distance to a given center.

    :param array_size: size of square ndarray to return
    :param center: dict of x,y coordinate of center
    :return: square ndarray of values equal to element's distance to center.
    """

    squared = {}
    start = {}
    end = {}

    output_array = np.zeros([array_size, array_size], dtype=float)

    for c in ['x', 'y']:
        squared[c] = (np.arange(array_size, dtype=float) - center[c]) ** 2
        start[c] = int(center[c] - radius)
        end[c] = int(center[c] + radius)
        if end[c] >= array_size:
            end[c] = array_size
        if start[c] < 0:
            start[c] = 0

    for x in range(start['x'], end['x']):
        for y in range(start['y'], end['y']):
            output_array[x, y] = np.sqrt(squared['x'][x] + squared['y'][y])

    return output_array


def _psf_phot(pixel_data, aperture, center, sky_value, n_pixels):
    """
    Simple aperture photometry of a numpy array of values.
    For use in calculating the ideal psf for a Strehl calculation

    :param pixel_data: an array containing pixel values
    :param aperture: pixel radius of aperture to use
    :param center: x,y coordinates of center of aperture
    :param sky_value: value of sky background
    :param n_pixels: number of pixels in one coordinate
    :return: total flux within aperture and peak flux
    """

    phot = 0
    values = [0]

    if np.size(aperture) > 1:
        aperture = np.ceil(aperture[0])

    radial_array = _dist_circle(n_pixels, center, aperture)

    (x, y) = np.where((radial_array <= aperture) & (radial_array != 0.0))

    num_elements = len(x)

    if num_elements > 0:
        for k in range(0, num_elements):
            values.append(pixel_data[x[k], y[k]] - sky_value)
            phot += (pixel_data[x[k], y[k]] - sky_value)

    return phot, max(values)


def _save_pupil(pupil, pupil_key, PUPIL_FILE):
    """
    Save the calculated pupil to avoid redundant calculations

    :param pupil_key: a combination of instrument, filter, and number of pixels
    :param pupil: an nd array of pupil to save
    :param PUPIL_FILE: Pupil file name to save
    """

    vals_to_save = {}
    try:
        file_data = np.load(PUPIL_FILE)
        for val in file_data.files:
            vals_to_save[val] = file_data[val]
    except IOError:
        pass

    vals_to_save[pupil_key] = pupil

    np.savez(PUPIL_FILE, **vals_to_save)
